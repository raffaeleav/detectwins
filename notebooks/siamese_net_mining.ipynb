{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7046cde1",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "# si aggiunge al path la cartella utils per avere visibilità del module\n",
        "module_path = Path(os.getcwd()).parent.parent\n",
        "module_path = os.path.join(module_path, \"project-detective\")\n",
        "\n",
        "sys.path.append(module_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "629f9a2a-51d3-4851-b6f2-a542601bb67e",
      "metadata": {
        "id": "629f9a2a-51d3-4851-b6f2-a542601bb67e",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import timm\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "import utils.mining as mining\n",
        "\n",
        "from torch import nn\n",
        "from tqdm import tqdm\n",
        "from skimage import io\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from pytorch_metric_learning import miners, losses\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fb6e0d0-b8cc-44b8-a8ac-17934de905f6",
      "metadata": {
        "id": "0fb6e0d0-b8cc-44b8-a8ac-17934de905f6",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# serve per ricaricare il codice modificato\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f02ae726-f601-4b51-a949-71a5464ec779",
      "metadata": {
        "id": "f02ae726-f601-4b51-a949-71a5464ec779",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# configurazione\n",
        "batch_size=32\n",
        "lr=0.001\n",
        "epochs=30\n",
        "device=\"cpu\"\n",
        "\n",
        "# per far funzionare il modello su immagini rgb o in scala di grigi (per usare fourier)\n",
        "mode=\"rgb\"\n",
        "\n",
        "# margin per semi-hard mining con modello pre-allenato\n",
        "margin=0.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bd2e0e4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# directory da dove vengono prelevate le immagini\n",
        "path = Path(os.getcwd()).parent.parent\n",
        "\n",
        "fake_data_dir = os.path.join(path, \"artifact\", \"taming_transformer\")\n",
        "real_data_dir = os.path.join(path, \"artifact\", \"coco\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa3dfd88-2685-4ef3-8cac-2e2c85c6f0fe",
      "metadata": {
        "id": "aa3dfd88-2685-4ef3-8cac-2e2c85c6f0fe",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# carica le immagini nel dataset\n",
        "class ApnDataset(Dataset):\n",
        "\n",
        "  def __init__(self, df):\n",
        "    self.df = df\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.df)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    row = self.df.iloc[idx]\n",
        "    \n",
        "    if mode == \"rgb\":\n",
        "      # le immagini Anchor sono memorizzate in due dataset diversi\n",
        "      if str(row.Anchor).startswith(\"coco\"):\n",
        "        a_img = io.imread(os.path.join(real_data_dir, row.Anchor))\n",
        "        p_img = io.imread(os.path.join(real_data_dir, row.Positive))\n",
        "        n_img = io.imread(os.path.join(fake_data_dir, row.Negative))\n",
        "\n",
        "        a_label = 0\n",
        "        p_label = 0\n",
        "        n_label = 1\n",
        "\n",
        "      else:\n",
        "        a_img = io.imread(os.path.join(fake_data_dir, row.Anchor))\n",
        "        p_img = io.imread(os.path.join(fake_data_dir, row.Positive))\n",
        "        n_img = io.imread(os.path.join(real_data_dir, row.Negative))\n",
        "\n",
        "        a_label = 1\n",
        "        p_label = 1\n",
        "        n_label = 0\n",
        "\n",
        "      # normalizzazione per immagini in rgb \n",
        "      a_img = torch.from_numpy(a_img).permute(2, 0, 1) / 255.0\n",
        "      p_img = torch.from_numpy(p_img).permute(2, 0, 1) / 255.0\n",
        "      n_img = torch.from_numpy(n_img).permute(2, 0, 1) / 255.0\n",
        "\n",
        "      a_label = torch.tensor(a_label)\n",
        "      p_label = torch.tensor(p_label)\n",
        "      n_label = torch.tensor(n_label)\n",
        "\n",
        "    if mode == \"grey_scale\":\n",
        "      a_img = np.expand_dims(a_img, 0)\n",
        "      p_img = np.expand_dims(p_img, 0)\n",
        "      n_img = np.expand_dims(n_img, 0)\n",
        "      \n",
        "      a_img = torch.from_numpy(a_img) / 255.0\n",
        "      p_img = torch.from_numpy(p_img) / 255.0\n",
        "      n_img = torch.from_numpy(n_img) / 255.0\n",
        "\n",
        "    # A_img = torch.from_numpy(A_img.astype(np.int32)) / 65536.0\n",
        "    # P_img = torch.from_numpy(P_img.astype(np.int32)) / 65536.0\n",
        "    # N_img = torch.from_numpy(N_img.astype(np.int32)) / 65536.0\n",
        "\n",
        "    return a_img, p_img, n_img, a_label, p_label, n_label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "477f08a1-15bb-471f-974c-23c6bef4ece3",
      "metadata": {
        "id": "477f08a1-15bb-471f-974c-23c6bef4ece3",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# classe per caricare il modello di rete neurale direttamente dalle repository online\n",
        "class ApnModel(nn.Module):\n",
        "\n",
        "  # size del vettore di embedding\n",
        "  def __init__(self, emb_size=512):\n",
        "    super(ApnModel, self).__init__()\n",
        "\n",
        "    # caricamento del modello, in questo caso efficientnet b0 (architettura più leggera della famiglia)\n",
        "    self.efficientnet = timm.create_model(\"tf_efficientnetv2_b1\", pretrained=False)\n",
        "    self.efficientnet.classifier = nn.Linear(in_features=self.efficientnet.classifier.in_features, out_features=emb_size)\n",
        "\n",
        "  def forward(self, images):\n",
        "    embeddings = self.efficientnet(images)\n",
        "    return embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53d21354",
      "metadata": {},
      "outputs": [],
      "source": [
        "# classe del modello che genera gli embedding per applicare il semi-hard mining\n",
        "class EmbModel(nn.Module):\n",
        "\n",
        "    # size del vettore di embedding\n",
        "    def __init__(self, emb_size = 512):\n",
        "        super(EmbModel, self).__init__()\n",
        "\n",
        "        # gli embedding vengono creati con un modello preallenato (risultato più efficace in test precedenti)\n",
        "        self.efficientnet = timm.create_model(\"tf_efficientnetv2_b0\", pretrained=True)\n",
        "        self.efficientnet.classifier = nn.Linear(in_features=self.efficientnet.classifier.in_features, out_features=emb_size)\n",
        "\n",
        "    def forward(self, images):\n",
        "        embeddings = self.efficientnet(images)\n",
        "        return embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c37a66e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# funzione per creare embeddings che sarranno sottoposti a semi-hard mining\n",
        "def create_embeddings(model, dataloader, device): \n",
        "    # off dropout\n",
        "    model.eval()\n",
        "\n",
        "    list_df = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for a, p, n, al, pl, nl in tqdm(dataloader, desc=\"creating embeddings...\"):\n",
        "            a, p, n = a.to(device), p.to(device), n.to(device)\n",
        "\n",
        "            temp_df_embs = pd.DataFrame(columns=[\"Anchor_embs\", \"Positive_embs\", \"Negative_embs\"])\n",
        "\n",
        "            a_embs = model(a)\n",
        "            p_embs = model(p)\n",
        "            n_embs = model(n)\n",
        "            \n",
        "            # la batch size può variare, perciò ci si basa sulla lunghezza del tensore\n",
        "            batch_size = len(a_embs)\n",
        "            \n",
        "            # ad ogni batch corrisponde un dataframe\n",
        "            for i in range(batch_size): \n",
        "                # si serializzano gli array np in stringhe in modo da memorizzarli nelle celle del datagrame\n",
        "                a, p, n = a_embs[i].cpu().numpy(), p_embs[i].cpu().numpy(), n_embs[i].cpu().numpy()\n",
        "                a, p, n = np.array2string(a, separator=','), np.array2string(p, separator=','), np.array2string(n, separator=',')\n",
        "                \n",
        "                temp_df_embs.loc[i] = [\n",
        "                    a, \n",
        "                    p, \n",
        "                    n\n",
        "                ]\n",
        "            \n",
        "            list_df.append(temp_df_embs)\n",
        "\n",
        "    # concatenazione di tutti i dataframe\n",
        "    df_embs = pd.concat(list_df)\n",
        "\n",
        "    return df_embs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4653903a",
      "metadata": {},
      "outputs": [],
      "source": [
        "emb_model = EmbModel()\n",
        "\n",
        "# per processare le immagini in scala di grigi per fare fourier serve una CNN 2D\n",
        "if mode == \"grey_scale\":\n",
        "    emb_model.efficientnet.conv_stem = nn.Conv2d(1, 32, 3, 2, 1, bias=False)\n",
        "\n",
        "emb_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c584653",
      "metadata": {},
      "outputs": [],
      "source": [
        "path = Path(os.getcwd()).parent.parent\n",
        "fake_dataset_path = os.path.join(path, \"artifact\", \"taming_transformer\", \"metadata.csv\")\n",
        "real_dataset_path = os.path.join(path, \"artifact\", \"coco\", \"metadata.csv\")\n",
        "\n",
        "df_out_path = os.path.join(\"..\", \"datasets\", \"out.csv\")\n",
        "df_out = pd.read_csv(df_out_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "012654d0",
      "metadata": {},
      "outputs": [],
      "source": [
        "apn_dataset = ApnDataset(df_out)\n",
        "dataloader = DataLoader(apn_dataset, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22b9da75",
      "metadata": {},
      "outputs": [],
      "source": [
        "emb_csv_path = os.path.join(\"..\", \"notebooks\", \"embeddings.csv\")\n",
        "\n",
        "# si controlla che siano stati già creati gli embeddings\n",
        "if not Path(emb_csv_path).is_file():\n",
        "    df_emb = create_embeddings(emb_model, dataloader, device)\n",
        "    df_emb.to_csv(emb_csv_path, index=False)\n",
        "\n",
        "df_emb = pd.read_csv(emb_csv_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41b31e13",
      "metadata": {},
      "outputs": [],
      "source": [
        "# si concatenano i dataframe delle immagini e degli embeddings sulle colonne per poter filtrare le righe in logica di semi-hard mining\n",
        "df_out = pd.concat([df_out, df_emb], axis=1)\n",
        "\n",
        "# offline semi-hard mining dei triplets\n",
        "df_out = mining.offline_semi_hard_mining(df_out, margin)\n",
        "df_out = df_out.drop([\"Anchor_embs\", \"Positive_embs\", \"Negative_embs\"], axis=1)\n",
        "\n",
        "print(f\"dataset size after semi-hard mining: {len(df_out)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28f6fa06-7584-4e35-844b-a1831a13c172",
      "metadata": {
        "id": "28f6fa06-7584-4e35-844b-a1831a13c172",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# funzione di train\n",
        "def train_fn(model, dataloader, optimizer, criterion, miner):\n",
        "  # on dropout \n",
        "  model.train()\n",
        "  \n",
        "  total_loss = 0.0\n",
        "\n",
        "  for a, p, n, al, pl, nl in tqdm(dataloader, desc=\"model training...\"):\n",
        "    a, p, n = a.to(device), p.to(device), n.to(device)\n",
        "    al, pl, nl = al.to(device), pl.to(device), nl.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # qui vengono creati gli embeddings, le cui distanze verranno calcolate dopo\n",
        "    a_embs = model(a)\n",
        "    p_embs = model(p)\n",
        "    n_embs = model(n)\n",
        "\n",
        "    # per usare l'ohm si devono concatenare tutti i tipi di immagine, i triplet verranno creati nella funzione di loss\n",
        "    embeddings = torch.cat((a_embs, p_embs, n_embs), axis=0)\n",
        "    labels = torch.cat((al, pl, nl), axis=0)\n",
        "\n",
        "    # online hard mining prima del calcolo della loss\n",
        "    miner_output = miner(embeddings, labels)\n",
        "    loss = criterion(embeddings, labels, miner_output)\n",
        "    loss.backward()\n",
        "    \n",
        "    optimizer.step()\n",
        "\n",
        "    total_loss += loss.item()\n",
        "\n",
        "  return total_loss / len(dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19ec6d56-9168-4980-9164-62660537f1ff",
      "metadata": {
        "id": "19ec6d56-9168-4980-9164-62660537f1ff",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# funzione di evaluation\n",
        "def eval_fn(model, dataloader, criterion):\n",
        "  # off dropout\n",
        "  model.eval() \n",
        "  \n",
        "  total_loss = 0.0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for a, p, n, al, pl, nl in tqdm(dataloader, desc=\"model validating...\"):\n",
        "      a, p, n = a.to(device), p.to(device), n.to(device)\n",
        "      al, pl, nl = al.to(device), pl.to(device), nl.to(device)\n",
        "\n",
        "      a_embs = model(a)\n",
        "      p_embs = model(p)\n",
        "      n_embs = model(n)\n",
        "\n",
        "      embeddings = torch.cat((a_embs, p_embs, n_embs), axis=0)\n",
        "      labels = torch.cat((al, pl, nl), axis=0)\n",
        "\n",
        "      loss = criterion(embeddings, labels)\n",
        "\n",
        "      total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2080916",
      "metadata": {},
      "outputs": [],
      "source": [
        "model = ApnModel()\n",
        "\n",
        "# per processare le immagini in scala di grigi per fare fourier serve una CNN 2D\n",
        "if mode == \"grey_scale\":\n",
        "    model.efficientnet.conv_stem = nn.Conv2d(1, 32, 3, 2, 1, bias=False)\n",
        "\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "311bed90",
      "metadata": {},
      "outputs": [],
      "source": [
        "# split del nuovo dataframe\n",
        "train_df, valid_df = train_test_split(df_out, test_size=0.20, random_state=42)\n",
        "\n",
        "trainset = ApnDataset(train_df)\n",
        "validset = ApnDataset(valid_df)\n",
        "\n",
        "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
        "validloader = DataLoader(validset, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f270e395-4b3f-4907-8ae7-a33043da0f80",
      "metadata": {
        "id": "f270e395-4b3f-4907-8ae7-a33043da0f80",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# triplet loss, miner (per online hard mining) e adam\n",
        "criterion = losses.TripletMarginLoss(triplets_per_anchor=\"all\")\n",
        "miner = miners.TripletMarginMiner(margin=margin, type_of_triplets=\"hard\")\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac5b64b5-d28c-47bc-bbbf-acd4960e34ef",
      "metadata": {
        "id": "ac5b64b5-d28c-47bc-bbbf-acd4960e34ef",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# training\n",
        "best_valid_loss = np.Inf\n",
        "\n",
        "training_epoch_loss = []\n",
        "validation_epoch_loss = []\n",
        "\n",
        "for i in range(epochs):\n",
        "  train_loss = train_fn(model, trainloader, optimizer, criterion, miner)\n",
        "  valid_loss = eval_fn(model, validloader, criterion)\n",
        "\n",
        "  training_epoch_loss.append(train_loss)\n",
        "  validation_epoch_loss.append(valid_loss)\n",
        "\n",
        "  if valid_loss < best_valid_loss:\n",
        "    torch.save(model.state_dict(), \"best_model.pt\")\n",
        "    best_valid_loss = valid_loss\n",
        "    print(\"successful weights saving...\")\n",
        "\n",
        "  print(f\"epochs: {i+1}, train_loss: {train_loss}, valid_loss: {valid_loss}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ca40d35",
      "metadata": {},
      "outputs": [],
      "source": [
        "# plot della training e validation loss\n",
        "plt.plot(training_epoch_loss, label=\"train_loss\")\n",
        "plt.plot(validation_epoch_loss, label=\"val_loss\")\n",
        "plt.legend()\n",
        "\n",
        "plt.show"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fe76305-8f2d-4159-a8e0-a57cb85b525e",
      "metadata": {
        "id": "9fe76305-8f2d-4159-a8e0-a57cb85b525e",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# funzione per generare i vettori di encoding\n",
        "def get_encoding_csv(model, anc_img_names, dir_folder):\n",
        "  anc_img_names_arr = np.array(anc_img_names)\n",
        "  encodings = []\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for i in tqdm(anc_img_names_arr, desc=\"creating encodings...\"):\n",
        "\n",
        "      if mode == \"rgb\":\n",
        "        # serve per trovare correttamente l'immagine\n",
        "        if str(i).startswith(\"coco\"):\n",
        "          dir_folder = real_data_dir\n",
        "          a = io.imread(os.path.join(dir_folder, i))\n",
        "\n",
        "        else: \n",
        "          dir_folder = fake_data_dir\n",
        "          a = io.imread(os.path.join(dir_folder, i))\n",
        "\n",
        "        a = torch.from_numpy(a).permute(2, 0, 1) / 255.0\n",
        "      \n",
        "      if mode == \"grey_scale\":\n",
        "        a = io.imread(os.path.join(dir_folder, i))\n",
        "        a = np.expand_dims(a, 0)\n",
        "        a = torch.from_numpy(a.astype(np.int32)) / 255.0\n",
        "        \n",
        "      a = a.to(device)\n",
        "      a_enc = model(a.unsqueeze(0))\n",
        "      encodings.append(a_enc.squeeze().cpu().detach().numpy())\n",
        "\n",
        "    encodings = np.array(encodings)\n",
        "    encodings = pd.DataFrame(encodings)\n",
        "    df_enc = pd.concat([anc_img_names, encodings], axis = 1)\n",
        "\n",
        "    return df_enc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "087c6f6f-3d8d-437f-ab31-ce2c3a1c239c",
      "metadata": {
        "id": "087c6f6f-3d8d-437f-ab31-ce2c3a1c239c",
        "outputId": "10e29b3a-1d0f-41bb-e9a2-21aec49dac69",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# per ricaricare il modello una volta allenato\n",
        "model.load_state_dict(torch.load(\"best_model.pt\"))\n",
        "\n",
        "# si creano gli embeddings che vengono memorizzati per non rifarlo ad ogni allenamento\n",
        "df_enc = get_encoding_csv(model, df_out[\"Anchor\"], real_data_dir)\n",
        "df_enc.to_csv(\"database.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a1096d8-a3dc-46bd-bb54-ed3debea3c57",
      "metadata": {
        "id": "5a1096d8-a3dc-46bd-bb54-ed3debea3c57",
        "outputId": "171dab62-2058-470c-9abf-5ea9495da9b0",
        "tags": []
      },
      "outputs": [],
      "source": [
        "df_enc = pd.read_csv('database.csv')\n",
        "df_enc.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea4d49ef-7e4b-4a09-a188-d6afa1fc273d",
      "metadata": {
        "id": "ea4d49ef-7e4b-4a09-a188-d6afa1fc273d",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# approssimazione della distanza, senza la radice quadrata, per fare i primi allenamenti velocemente\n",
        "def euclidean_dist(img_enc, anc_enc_arr):\n",
        "    # dist = np.sqrt(np.dot(img_enc-anc_enc_arr, (img_enc- anc_enc_arr).T))\n",
        "    dist = np.dot(img_enc - anc_enc_arr, (img_enc - anc_enc_arr).T)\n",
        "    # dist = np.sqrt(dist)\n",
        "    return dist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38e0d751-6559-43b8-94f1-eab11f754bdd",
      "metadata": {
        "id": "38e0d751-6559-43b8-94f1-eab11f754bdd",
        "outputId": "7ff19abf-6ff7-4f31-bd3e-a07d07ca90dd",
        "tags": []
      },
      "outputs": [],
      "source": [
        "test_df_path = os.path.join(\"..\", \"datasets\", \"testList.csv\")\n",
        "test_df = pd.read_csv(test_df_path)\n",
        "\n",
        "print(test_df[\"real\"])\n",
        "print(test_df.size)\n",
        "test_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2262d0bc-a58d-4663-8515-bccbdf870608",
      "metadata": {
        "id": "2262d0bc-a58d-4663-8515-bccbdf870608",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def get_image_embeddings(img, model):\n",
        "    if mode == \"rgb\":\n",
        "        img = torch.from_numpy(img).permute(2, 0, 1) / 255.0\n",
        "      \n",
        "    if mode == \"grey_scale\":\n",
        "        img = np.expand_dims(img, 0)\n",
        "        img = torch.from_numpy(img) / 255\n",
        "    \n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        img = img.to(device)\n",
        "        img_enc = model(img.unsqueeze(0))\n",
        "        img_enc = img_enc.detach().cpu().numpy()\n",
        "        img_enc = np.array(img_enc)\n",
        "\n",
        "    return img_enc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b4b5bd4-222b-44a3-b32d-2aa1f9b3d405",
      "metadata": {
        "id": "0b4b5bd4-222b-44a3-b32d-2aa1f9b3d405",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def search_in_database(img_enc, database):\n",
        "    anc_enc_arr = database.iloc[:, 1:].to_numpy()\n",
        "    anc_img_names = database[\"Anchor\"]\n",
        "\n",
        "    distance = []\n",
        "    for i in range(anc_enc_arr.shape[0]):\n",
        "        dist = euclidean_dist(img_enc, anc_enc_arr[i : i+1, :])\n",
        "        distance = np.append(distance, dist)\n",
        "\n",
        "    closest_idx = np.argsort(distance)\n",
        "\n",
        "    return database[\"Anchor\"][closest_idx[0]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddd32725-5e2d-42a7-8998-4920f05ed143",
      "metadata": {
        "id": "ddd32725-5e2d-42a7-8998-4920f05ed143",
        "outputId": "888e6f94-a62a-46e1-cf29-d11664da20b7",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# DataTestReal = 'C:/Users/polsi/Desktop/Lavori/DeepFake/Datasets/Artifact/cycle_gan/st/test/'\n",
        "path = Path(os.getcwd()).parent.parent\n",
        "real_dataset_dir = os.path.join(path, \"artifact\", \"coco\")\n",
        "fake_dataset_dir = os.path.join(path, \"artifact\", \"taming_transformer\")\n",
        "\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "temp_df = test_df\n",
        "temp_df.head()\n",
        "temp_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca7392f6-109f-4a96-92b1-e85600be8d8a",
      "metadata": {
        "id": "ca7392f6-109f-4a96-92b1-e85600be8d8a",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# testo i fake\n",
        "current_test = \"fake\"\n",
        "database = df_enc\n",
        "\n",
        "# prendo i primi 500 Fake\n",
        "for index, row in tqdm(temp_df.iterrows(), desc=\"testing on fake images...\"):\n",
        "    path = os.path.join(fake_dataset_dir, row[current_test])\n",
        "    img_name = path\n",
        "\n",
        "    img = io.imread(img_name)\n",
        "    img_enc = get_image_embeddings(img, model)\n",
        "    closest_label = search_in_database(img_enc, database)\n",
        "\n",
        "    if mode == \"rgb\":\n",
        "        if str(closest_label).startswith(\"coco\"):\n",
        "            y_pred.append(\"real\")\n",
        "        else:\n",
        "            y_pred.append(\"fake\")\n",
        "\n",
        "    if mode == \"grey_scale\": \n",
        "        if \"real\" in closest_label:\n",
        "            y_pred.append(\"real\")\n",
        "        else:\n",
        "            y_pred.append(\"fake\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe3af5ae-b8c2-419e-b5a5-9d17609797f5",
      "metadata": {
        "id": "fe3af5ae-b8c2-419e-b5a5-9d17609797f5",
        "tags": []
      },
      "outputs": [],
      "source": [
        "print(len(y_true))\n",
        "print(len(y_pred))\n",
        "print(y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46b8f2c1-a3ca-4bbd-8326-02685fc44cf8",
      "metadata": {
        "id": "46b8f2c1-a3ca-4bbd-8326-02685fc44cf8",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# testo i real\n",
        "current_test = \"real\"\n",
        "database = df_enc\n",
        "\n",
        "# prendo i primi 500 Fake\n",
        "for index, row in tqdm(temp_df.iterrows(), desc=\"testing on real images...\"):\n",
        "    path = os.path.join(real_dataset_dir, row[current_test])\n",
        "    img_name = path\n",
        "\n",
        "    img = io.imread(img_name)\n",
        "    img_enc = get_image_embeddings(img, model)\n",
        "    closest_label = search_in_database(img_enc, database)\n",
        "    \n",
        "    if mode == \"rgb\":\n",
        "        if str(closest_label).startswith(\"coco\"):\n",
        "            y_pred.append(\"real\")\n",
        "        else:\n",
        "            y_pred.append(\"fake\")\n",
        "\n",
        "    if mode == \"grey_scale\":\n",
        "        if \"real\" in closest_label:\n",
        "            y_pred.append(\"real\")\n",
        "        else:\n",
        "            y_pred.append(\"fake\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c465bfd-18ad-4750-b689-739b712185ab",
      "metadata": {
        "id": "4c465bfd-18ad-4750-b689-739b712185ab",
        "outputId": "e974c712-91fb-4fae-c589-85e08a50fb77",
        "tags": []
      },
      "outputs": [],
      "source": [
        "print(len(y_true))\n",
        "print(len(y_pred))\n",
        "print(y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85706e81-3068-4150-9773-320a8aa98c69",
      "metadata": {
        "id": "85706e81-3068-4150-9773-320a8aa98c69",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# creo i vettori di ground truth\n",
        "y_true = np.array([\"fake\"] * len(valid_df))\n",
        "print(y_true.shape)\n",
        "\n",
        "temp = np.array([\"real\"] * len(valid_df))\n",
        "print(temp.shape)\n",
        "\n",
        "y_true = np.concatenate([y_true, temp])\n",
        "print(y_true.shape)\n",
        "\n",
        "# calcolo la matrice di confusione (quella di scikit-learn dispone i risultati come nella cella di sotto)\n",
        "cm = confusion_matrix(y_true, y_pred, labels=[\"real\", \"fake\"])\n",
        "print(cm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28d903d0-38e3-4dc7-af71-f091c78a00e7",
      "metadata": {
        "id": "28d903d0-38e3-4dc7-af71-f091c78a00e7",
        "tags": []
      },
      "outputs": [],
      "source": [
        "tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "# metriche\n",
        "accuracy = round((tp + tn) / (tp + tn + fp + fn), 4) * 100\n",
        "precision = round((tp) / (tp + fp), 4) * 100\n",
        "recall = round((tp) / (tp + fn), 4) * 100\n",
        "specificity = round((tn) / (tn + fp) * 100, 4)\n",
        "f1_score = round((2 * precision * recall) / (precision + recall), 4)\n",
        "\n",
        "print({\"Accuracy\":accuracy, \"Precision\":precision, \"Recall\":recall, \"Specificity\":specificity, \"F1 Score\":f1_score})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb6aac2a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# si salvano i risultati in un file .csv\n",
        "df_results = pd.DataFrame(columns=[\"Accuracy\", \"Precision\", \"Recall\", \"Specificity\", \"F1 Score\"])\n",
        "df_results.loc[0] = [accuracy, precision, recall, specificity, f1_score]\n",
        "\n",
        "# si differenziano i risultati in base al tipo di immagini e dataset usati\n",
        "dataset = fake_data_dir.split(\"\\\\\")[-1]\n",
        "path = os.path.join(\"..\", \"results\", \"rgb_mining\", \"siamese_\" + mode + \"_\" + \"pretrained_semi_hard_online_hard_\" + dataset + \"_results.csv\")\n",
        "\n",
        "df_results.to_csv(path, index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "fvab",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
