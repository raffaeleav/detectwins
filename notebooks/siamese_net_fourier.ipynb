{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "629f9a2a-51d3-4851-b6f2-a542601bb67e",
   "metadata": {
    "id": "629f9a2a-51d3-4851-b6f2-a542601bb67e",
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-05-07T17:19:12.035797Z",
     "start_time": "2024-05-07T17:19:11.839796Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import sys\n",
    "import timm\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from skimage import io\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0fb6e0d0-b8cc-44b8-a8ac-17934de905f6",
   "metadata": {
    "id": "0fb6e0d0-b8cc-44b8-a8ac-17934de905f6",
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-05-07T17:19:12.207801Z",
     "start_time": "2024-05-07T17:19:12.037798Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# serve per ricaricare automaticamente il codice modificato\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f02ae726-f601-4b51-a949-71a5464ec779",
   "metadata": {
    "id": "f02ae726-f601-4b51-a949-71a5464ec779",
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-05-07T17:19:12.379811Z",
     "start_time": "2024-05-07T17:19:12.210802Z"
    }
   },
   "outputs": [],
   "source": [
    "# directory principale di training, perche' nel file .csv ci sono solo i nomi dei file (immagini), da modificare per \n",
    "path = Path(os.getcwd()).parent.parent\n",
    "real_data_dir = os.path.join(path, \"artifact\", \"coco\")\n",
    "fake_data_dir = os.path.join(path, \"artifact\", \"latent_diffusion\")\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "LR = 0.001\n",
    "\n",
    "EPOCHS = 30\n",
    "\n",
    "DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6c035a79-a777-4fb4-83fc-c71f1f4d37a7",
   "metadata": {
    "id": "6c035a79-a777-4fb4-83fc-c71f1f4d37a7",
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-05-07T17:19:12.523810Z",
     "start_time": "2024-05-07T17:19:12.382795Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19200\n",
      "4800\n"
     ]
    }
   ],
   "source": [
    "# df che devo creare\n",
    "csv_path = os.path.join(\"..\", \"datasets\", \"out.csv\")\n",
    "df = pd.read_csv(csv_path)\n",
    "#df.head()\n",
    "df = df.sample(frac=1)\n",
    "train_df, valid_df = train_test_split(df, test_size=0.20, random_state=42)\n",
    "\n",
    "print(train_df.size)\n",
    "print(valid_df.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa3dfd88-2685-4ef3-8cac-2e2c85c6f0fe",
   "metadata": {
    "id": "aa3dfd88-2685-4ef3-8cac-2e2c85c6f0fe",
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-05-07T17:19:12.667809Z",
     "start_time": "2024-05-07T17:19:12.526796Z"
    }
   },
   "outputs": [],
   "source": [
    "# carica le immagini nel dataset\n",
    "class APN_Dataset(Dataset):\n",
    "\n",
    "  def __init__(self, df):\n",
    "    self.df = df\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.df)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    row = self.df.iloc[idx]\n",
    "    \n",
    "    #2   0_img = io.imread(os.path.join(real_data_dir, row.Anchor))\n",
    "    #P_img = io.imread(os.path.join(real_data_dir, row.Positive))\n",
    "    #N_img = io.imread(os.path.join(fake_data_dir, row.Negative))\n",
    "    \n",
    "    \n",
    "    if \"coco\" in row.Anchor:\n",
    "        A_img = io.imread(os.path.join(real_data_dir, row.Anchor))\n",
    "        P_img = io.imread(os.path.join(real_data_dir, row.Positive))\n",
    "        N_img = io.imread(os.path.join(fake_data_dir, row.Negative))\n",
    "    \n",
    "    else:\n",
    "        A_img = io.imread(os.path.join(fake_data_dir, row.Anchor))\n",
    "        P_img = io.imread(os.path.join(fake_data_dir, row.Positive))\n",
    "        N_img = io.imread(os.path.join(real_data_dir, row.Negative))\n",
    "\n",
    "    # permute because the third channel has to be in first channel in torch\n",
    "\n",
    "    A_img = torch.from_numpy(A_img).permute(2, 0, 1) / 255.0\n",
    "    P_img = torch.from_numpy(P_img).permute(2, 0, 1) / 255.0\n",
    "    N_img = torch.from_numpy(N_img).permute(2, 0, 1) / 255.0\n",
    "    \n",
    "    #A_img = np.expand_dims(A_img, 0)\n",
    "    #P_img = np.expand_dims(P_img, 0)\n",
    "    #N_img = np.expand_dims(N_img, 0)\n",
    "    \n",
    "    # normalizzazione per non far divergere il comportamento della rete\n",
    "    # il valore dell'immagine sarà compreso tra 0 e 1\n",
    "    #A_img = torch.from_numpy(A_img) / 255.0\n",
    "    #P_img = torch.from_numpy(P_img) / 255.0\n",
    "    #N_img = torch.from_numpy(N_img) / 255.0\n",
    "\n",
    "    #A_img = torch.from_numpy(A_img.astype(np.int32)) / 65536.0\n",
    "    #P_img = torch.from_numpy(P_img.astype(np.int32)) / 65536.0\n",
    "    #N_img = torch.from_numpy(N_img.astype(np.int32)) / 65536.0\n",
    "\n",
    "    return A_img, P_img, N_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8203b726-6dcc-41fc-ac33-d98f58e61ac7",
   "metadata": {
    "id": "8203b726-6dcc-41fc-ac33-d98f58e61ac7",
    "outputId": "60287971-91ea-41f4-e564-ec8524cb8d3d",
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-05-07T17:19:12.808353Z",
     "start_time": "2024-05-07T17:19:12.669797Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of trainset: 6400\n",
      "Size of validset: 1600\n"
     ]
    }
   ],
   "source": [
    "trainset = APN_Dataset(train_df)\n",
    "validset = APN_Dataset(valid_df)\n",
    "\n",
    "print(f\"Size of trainset: {len(trainset)}\")\n",
    "print(f\"Size of validset: {len(validset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4fae8215-4694-4710-b55a-e9c10bf42690",
   "metadata": {
    "id": "4fae8215-4694-4710-b55a-e9c10bf42690",
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-05-07T17:19:12.950327Z",
     "start_time": "2024-05-07T17:19:12.811300Z"
    }
   },
   "outputs": [],
   "source": [
    "trainloader = DataLoader(trainset, batch_size = BATCH_SIZE, shuffle = True)\n",
    "validloader = DataLoader(validset, batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9b9c0301-e6a8-47cb-91f8-9850b78e39d5",
   "metadata": {
    "id": "9b9c0301-e6a8-47cb-91f8-9850b78e39d5",
    "outputId": "0be94549-1119-4856-dce2-a7df0229a36d",
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-05-07T17:19:13.075409Z",
     "start_time": "2024-05-07T17:19:12.952300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of batches in trainloader : 200\n",
      "No. of batches in validloader : 50\n"
     ]
    }
   ],
   "source": [
    "print(f\"No. of batches in trainloader : {len(trainloader)}\")\n",
    "print(f\"No. of batches in validloader : {len(validloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "477f08a1-15bb-471f-974c-23c6bef4ece3",
   "metadata": {
    "id": "477f08a1-15bb-471f-974c-23c6bef4ece3",
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-05-07T17:19:13.217317Z",
     "start_time": "2024-05-07T17:19:13.079302Z"
    }
   },
   "outputs": [],
   "source": [
    "# FUNZIONE PER CARICARE IL MODELLO DI RETE NEURALE DIRETTAMENTE DALLE REPOSITORY ONLINE\n",
    "class APN_Model(nn.Module):\n",
    "\n",
    "  # QUI DEFINISCO LA SIZE DEL VETTORE DI EMBEDDING\n",
    "  def __init__(self, emb_size = 512):\n",
    "    super(APN_Model, self).__init__()\n",
    "\n",
    "    # QUI CAIRCATE IL MODELLO, IN QUESTO CASO EFFICIENTNET VERSIONE B0 (LA PIù LEGGERA DELLA FAMIGLIA)\n",
    "    self.efficientnet = timm.create_model('tf_efficientnetv2_b0', pretrained=False)\n",
    "    self.efficientnet.classifier = nn.Linear(in_features=self.efficientnet.classifier.in_features, out_features=emb_size)\n",
    "\n",
    "  def forward(self, images):\n",
    "    embeddings = self.efficientnet(images)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a64f5032-dc37-4858-a901-d17b2eaed4db",
   "metadata": {
    "id": "a64f5032-dc37-4858-a901-d17b2eaed4db",
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-05-07T17:19:14.255078Z",
     "start_time": "2024-05-07T17:19:13.221308Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "APN_Model(\n  (efficientnet): EfficientNet(\n    (conv_stem): Conv2dSame(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n    (bn1): BatchNormAct2d(\n      32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n      (drop): Identity()\n      (act): SiLU(inplace=True)\n    )\n    (blocks): Sequential(\n      (0): Sequential(\n        (0): ConvBnAct(\n          (conv): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn1): BatchNormAct2d(\n            16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n          (drop_path): Identity()\n        )\n      )\n      (1): Sequential(\n        (0): EdgeResidual(\n          (conv_exp): Conv2dSame(16, 64, kernel_size=(3, 3), stride=(2, 2), bias=False)\n          (bn1): BatchNormAct2d(\n            64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n          (se): Identity()\n          (conv_pwl): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn2): BatchNormAct2d(\n            32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): Identity()\n          )\n          (drop_path): Identity()\n        )\n        (1): EdgeResidual(\n          (conv_exp): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn1): BatchNormAct2d(\n            128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n          (se): Identity()\n          (conv_pwl): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn2): BatchNormAct2d(\n            32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): Identity()\n          )\n          (drop_path): Identity()\n        )\n      )\n      (2): Sequential(\n        (0): EdgeResidual(\n          (conv_exp): Conv2dSame(32, 128, kernel_size=(3, 3), stride=(2, 2), bias=False)\n          (bn1): BatchNormAct2d(\n            128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n          (se): Identity()\n          (conv_pwl): Conv2d(128, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn2): BatchNormAct2d(\n            48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): Identity()\n          )\n          (drop_path): Identity()\n        )\n        (1): EdgeResidual(\n          (conv_exp): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn1): BatchNormAct2d(\n            192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n          (se): Identity()\n          (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn2): BatchNormAct2d(\n            48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): Identity()\n          )\n          (drop_path): Identity()\n        )\n      )\n      (3): Sequential(\n        (0): InvertedResidual(\n          (conv_pw): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNormAct2d(\n            192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n          (conv_dw): Conv2dSame(192, 192, kernel_size=(3, 3), stride=(2, 2), groups=192, bias=False)\n          (bn2): BatchNormAct2d(\n            192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n          (se): SqueezeExcite(\n            (conv_reduce): Conv2d(192, 12, kernel_size=(1, 1), stride=(1, 1))\n            (act1): SiLU(inplace=True)\n            (conv_expand): Conv2d(12, 192, kernel_size=(1, 1), stride=(1, 1))\n            (gate): Sigmoid()\n          )\n          (conv_pwl): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNormAct2d(\n            96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): Identity()\n          )\n          (drop_path): Identity()\n        )\n        (1): InvertedResidual(\n          (conv_pw): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNormAct2d(\n            384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n          (conv_dw): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n          (bn2): BatchNormAct2d(\n            384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n          (se): SqueezeExcite(\n            (conv_reduce): Conv2d(384, 24, kernel_size=(1, 1), stride=(1, 1))\n            (act1): SiLU(inplace=True)\n            (conv_expand): Conv2d(24, 384, kernel_size=(1, 1), stride=(1, 1))\n            (gate): Sigmoid()\n          )\n          (conv_pwl): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNormAct2d(\n            96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): Identity()\n          )\n          (drop_path): Identity()\n        )\n        (2): InvertedResidual(\n          (conv_pw): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNormAct2d(\n            384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n          (conv_dw): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n          (bn2): BatchNormAct2d(\n            384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n          (se): SqueezeExcite(\n            (conv_reduce): Conv2d(384, 24, kernel_size=(1, 1), stride=(1, 1))\n            (act1): SiLU(inplace=True)\n            (conv_expand): Conv2d(24, 384, kernel_size=(1, 1), stride=(1, 1))\n            (gate): Sigmoid()\n          )\n          (conv_pwl): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNormAct2d(\n            96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): Identity()\n          )\n          (drop_path): Identity()\n        )\n      )\n      (4): Sequential(\n        (0): InvertedResidual(\n          (conv_pw): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNormAct2d(\n            576, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n          (conv_dw): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n          (bn2): BatchNormAct2d(\n            576, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n          (se): SqueezeExcite(\n            (conv_reduce): Conv2d(576, 24, kernel_size=(1, 1), stride=(1, 1))\n            (act1): SiLU(inplace=True)\n            (conv_expand): Conv2d(24, 576, kernel_size=(1, 1), stride=(1, 1))\n            (gate): Sigmoid()\n          )\n          (conv_pwl): Conv2d(576, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNormAct2d(\n            112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): Identity()\n          )\n          (drop_path): Identity()\n        )\n        (1): InvertedResidual(\n          (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNormAct2d(\n            672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n          (conv_dw): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n          (bn2): BatchNormAct2d(\n            672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n          (se): SqueezeExcite(\n            (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n            (act1): SiLU(inplace=True)\n            (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n            (gate): Sigmoid()\n          )\n          (conv_pwl): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNormAct2d(\n            112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): Identity()\n          )\n          (drop_path): Identity()\n        )\n        (2): InvertedResidual(\n          (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNormAct2d(\n            672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n          (conv_dw): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n          (bn2): BatchNormAct2d(\n            672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n          (se): SqueezeExcite(\n            (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n            (act1): SiLU(inplace=True)\n            (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n            (gate): Sigmoid()\n          )\n          (conv_pwl): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNormAct2d(\n            112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): Identity()\n          )\n          (drop_path): Identity()\n        )\n        (3): InvertedResidual(\n          (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNormAct2d(\n            672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n          (conv_dw): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n          (bn2): BatchNormAct2d(\n            672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n          (se): SqueezeExcite(\n            (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n            (act1): SiLU(inplace=True)\n            (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n            (gate): Sigmoid()\n          )\n          (conv_pwl): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNormAct2d(\n            112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): Identity()\n          )\n          (drop_path): Identity()\n        )\n        (4): InvertedResidual(\n          (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNormAct2d(\n            672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n          (conv_dw): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n          (bn2): BatchNormAct2d(\n            672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n          (se): SqueezeExcite(\n            (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n            (act1): SiLU(inplace=True)\n            (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n            (gate): Sigmoid()\n          )\n          (conv_pwl): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNormAct2d(\n            112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): Identity()\n          )\n          (drop_path): Identity()\n        )\n      )\n      (5): Sequential(\n        (0): InvertedResidual(\n          (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNormAct2d(\n            672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n          (conv_dw): Conv2dSame(672, 672, kernel_size=(3, 3), stride=(2, 2), groups=672, bias=False)\n          (bn2): BatchNormAct2d(\n            672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n          (se): SqueezeExcite(\n            (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n            (act1): SiLU(inplace=True)\n            (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n            (gate): Sigmoid()\n          )\n          (conv_pwl): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNormAct2d(\n            192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): Identity()\n          )\n          (drop_path): Identity()\n        )\n        (1): InvertedResidual(\n          (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNormAct2d(\n            1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n          (conv_dw): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n          (bn2): BatchNormAct2d(\n            1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n          (se): SqueezeExcite(\n            (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n            (act1): SiLU(inplace=True)\n            (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n            (gate): Sigmoid()\n          )\n          (conv_pwl): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNormAct2d(\n            192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): Identity()\n          )\n          (drop_path): Identity()\n        )\n        (2): InvertedResidual(\n          (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNormAct2d(\n            1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n          (conv_dw): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n          (bn2): BatchNormAct2d(\n            1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n          (se): SqueezeExcite(\n            (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n            (act1): SiLU(inplace=True)\n            (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n            (gate): Sigmoid()\n          )\n          (conv_pwl): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNormAct2d(\n            192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): Identity()\n          )\n          (drop_path): Identity()\n        )\n        (3): InvertedResidual(\n          (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNormAct2d(\n            1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n          (conv_dw): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n          (bn2): BatchNormAct2d(\n            1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n          (se): SqueezeExcite(\n            (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n            (act1): SiLU(inplace=True)\n            (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n            (gate): Sigmoid()\n          )\n          (conv_pwl): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNormAct2d(\n            192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): Identity()\n          )\n          (drop_path): Identity()\n        )\n        (4): InvertedResidual(\n          (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNormAct2d(\n            1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n          (conv_dw): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n          (bn2): BatchNormAct2d(\n            1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n          (se): SqueezeExcite(\n            (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n            (act1): SiLU(inplace=True)\n            (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n            (gate): Sigmoid()\n          )\n          (conv_pwl): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNormAct2d(\n            192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): Identity()\n          )\n          (drop_path): Identity()\n        )\n        (5): InvertedResidual(\n          (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNormAct2d(\n            1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n          (conv_dw): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n          (bn2): BatchNormAct2d(\n            1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n          (se): SqueezeExcite(\n            (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n            (act1): SiLU(inplace=True)\n            (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n            (gate): Sigmoid()\n          )\n          (conv_pwl): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNormAct2d(\n            192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): Identity()\n          )\n          (drop_path): Identity()\n        )\n        (6): InvertedResidual(\n          (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNormAct2d(\n            1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n          (conv_dw): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n          (bn2): BatchNormAct2d(\n            1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n          (se): SqueezeExcite(\n            (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n            (act1): SiLU(inplace=True)\n            (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n            (gate): Sigmoid()\n          )\n          (conv_pwl): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNormAct2d(\n            192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): Identity()\n          )\n          (drop_path): Identity()\n        )\n        (7): InvertedResidual(\n          (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNormAct2d(\n            1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n          (conv_dw): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n          (bn2): BatchNormAct2d(\n            1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): SiLU(inplace=True)\n          )\n          (se): SqueezeExcite(\n            (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n            (act1): SiLU(inplace=True)\n            (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n            (gate): Sigmoid()\n          )\n          (conv_pwl): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNormAct2d(\n            192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n            (drop): Identity()\n            (act): Identity()\n          )\n          (drop_path): Identity()\n        )\n      )\n    )\n    (conv_head): Conv2d(192, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (bn2): BatchNormAct2d(\n      1280, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n      (drop): Identity()\n      (act): SiLU(inplace=True)\n    )\n    (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n    (classifier): Linear(in_features=1280, out_features=512, bias=True)\n  )\n)"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# QUI FATE UNA PICCOLA MODIFICA ALLA RETE PER FARLE AVERE IN INPUT IMMAGINI IN SCALA DI GRIGIO DELLO SPETTRO DI FOURIER\n",
    "model = APN_Model()\n",
    "#model.efficientnet.conv_stem = nn.Conv2d(1, 32, 3, 2, 1, bias=False)\n",
    "\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "28f6fa06-7584-4e35-844b-a1831a13c172",
   "metadata": {
    "id": "28f6fa06-7584-4e35-844b-a1831a13c172",
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-05-07T17:19:14.463077Z",
     "start_time": "2024-05-07T17:19:14.259080Z"
    }
   },
   "outputs": [],
   "source": [
    "# funzione di train\n",
    "def train_fn(model, dataloader, optimizer, criterion):\n",
    "  model.train() #ON Dropout\n",
    "  total_loss = 0.0\n",
    "\n",
    "  for A, P, N in tqdm(dataloader):\n",
    "    A, P, N = A.to(DEVICE), P.to(DEVICE), N.to(DEVICE)\n",
    "\n",
    "    print(\"debug, A shape: \" + str(A.shape), file=sys.stderr)\n",
    "    print(\"debug, P shape: \" + str(P.shape), file=sys.stderr)\n",
    "    print(\"debug, N shape: \" + str(N.shape), file=sys.stderr)\n",
    "\n",
    "    # qui vengono creati gli embeddings, le cui distanze verranno calcolate dopo\n",
    "    A_embs = model(A)\n",
    "    P_embs = model(P)\n",
    "    N_embs = model(N)\n",
    "\n",
    "    # criterion è la funzione di loss triplet\n",
    "    loss = criterion(A_embs, P_embs, N_embs)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "19ec6d56-9168-4980-9164-62660537f1ff",
   "metadata": {
    "id": "19ec6d56-9168-4980-9164-62660537f1ff",
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-05-07T17:19:14.701082Z",
     "start_time": "2024-05-07T17:19:14.468081Z"
    }
   },
   "outputs": [],
   "source": [
    "# funzione di evaluation\n",
    "def eval_fn(model, dataloader, criterion):\n",
    "  model.eval() #OFF Dropout\n",
    "  total_loss = 0.0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for A, P, N in tqdm(dataloader):\n",
    "      A, P, N = A.to(DEVICE), P.to(DEVICE), N.to(DEVICE)\n",
    "\n",
    "      A_embs = model(A)\n",
    "      P_embs = model(P)\n",
    "      N_embs = model(N)\n",
    "\n",
    "      loss = criterion(A_embs, P_embs, N_embs)\n",
    "\n",
    "      total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f270e395-4b3f-4907-8ae7-a33043da0f80",
   "metadata": {
    "id": "f270e395-4b3f-4907-8ae7-a33043da0f80",
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-05-07T17:19:14.844196Z",
     "start_time": "2024-05-07T17:19:14.704081Z"
    }
   },
   "outputs": [],
   "source": [
    "# triplet loss e adam\n",
    "criterion = nn.TripletMarginLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ac5b64b5-d28c-47bc-bbbf-acd4960e34ef",
   "metadata": {
    "id": "ac5b64b5-d28c-47bc-bbbf-acd4960e34ef",
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-05-07T17:47:05.007967Z",
     "start_time": "2024-05-07T17:19:14.846139Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]debug, A shape: torch.Size([32, 3, 200, 200])\n",
      "debug, P shape: torch.Size([32, 3, 200, 200])\n",
      "debug, N shape: torch.Size([32, 3, 200, 200])\n",
      "  0%|          | 0/200 [00:04<?, ?it/s]\n",
      "100%|██████████| 50/50 [01:32<00:00,  1.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED_WEIGHTS_SUCCESS\n",
      "EPOCHS : 1 train_loss : 0.012199723720550537 valid_loss : 0.9999604296684265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]debug, A shape: torch.Size([32, 3, 200, 200])\n",
      "debug, P shape: torch.Size([32, 3, 200, 200])\n",
      "debug, N shape: torch.Size([32, 3, 200, 200])\n",
      "  0%|          | 0/200 [00:04<?, ?it/s]\n",
      "100%|██████████| 50/50 [00:50<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED_WEIGHTS_SUCCESS\n",
      "EPOCHS : 2 train_loss : 0.01056828498840332 valid_loss : 0.9998945987224579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]debug, A shape: torch.Size([32, 3, 200, 200])\n",
      "debug, P shape: torch.Size([32, 3, 200, 200])\n",
      "debug, N shape: torch.Size([32, 3, 200, 200])\n",
      "  0%|          | 0/200 [00:03<?, ?it/s]\n",
      "100%|██████████| 50/50 [00:50<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED_WEIGHTS_SUCCESS\n",
      "EPOCHS : 3 train_loss : 0.005008600950241089 valid_loss : 0.9997799372673035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]debug, A shape: torch.Size([32, 3, 200, 200])\n",
      "debug, P shape: torch.Size([32, 3, 200, 200])\n",
      "debug, N shape: torch.Size([32, 3, 200, 200])\n",
      "  0%|          | 0/200 [00:04<?, ?it/s]\n",
      "100%|██████████| 50/50 [00:49<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED_WEIGHTS_SUCCESS\n",
      "EPOCHS : 4 train_loss : 0.004730768203735351 valid_loss : 0.9997707581520081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]debug, A shape: torch.Size([32, 3, 200, 200])\n",
      "debug, P shape: torch.Size([32, 3, 200, 200])\n",
      "debug, N shape: torch.Size([32, 3, 200, 200])\n",
      "  0%|          | 0/200 [00:03<?, ?it/s]\n",
      "100%|██████████| 50/50 [00:48<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED_WEIGHTS_SUCCESS\n",
      "EPOCHS : 5 train_loss : 0.007531881928443908 valid_loss : 0.9992433297634125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]debug, A shape: torch.Size([32, 3, 200, 200])\n",
      "debug, P shape: torch.Size([32, 3, 200, 200])\n",
      "debug, N shape: torch.Size([32, 3, 200, 200])\n",
      "  0%|          | 0/200 [00:04<?, ?it/s]\n",
      "100%|██████████| 50/50 [00:49<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED_WEIGHTS_SUCCESS\n",
      "EPOCHS : 6 train_loss : 0.007550429105758667 valid_loss : 0.9970844340324402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]debug, A shape: torch.Size([32, 3, 200, 200])\n",
      "debug, P shape: torch.Size([32, 3, 200, 200])\n",
      "debug, N shape: torch.Size([32, 3, 200, 200])\n",
      "  0%|          | 0/200 [00:03<?, ?it/s]\n",
      "100%|██████████| 50/50 [00:49<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED_WEIGHTS_SUCCESS\n",
      "EPOCHS : 7 train_loss : 0.006054401993751526 valid_loss : 0.9965182542800903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]debug, A shape: torch.Size([32, 3, 200, 200])\n",
      "debug, P shape: torch.Size([32, 3, 200, 200])\n",
      "debug, N shape: torch.Size([32, 3, 200, 200])\n",
      "  0%|          | 0/200 [00:04<?, ?it/s]\n",
      "100%|██████████| 50/50 [00:48<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCHS : 8 train_loss : 0.0037985152006149293 valid_loss : 0.9997199487686157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]debug, A shape: torch.Size([32, 3, 200, 200])\n",
      "debug, P shape: torch.Size([32, 3, 200, 200])\n",
      "debug, N shape: torch.Size([32, 3, 200, 200])\n",
      "  0%|          | 0/200 [00:03<?, ?it/s]\n",
      "100%|██████████| 50/50 [00:48<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED_WEIGHTS_SUCCESS\n",
      "EPOCHS : 9 train_loss : 0.008998490571975708 valid_loss : 0.9945969188213348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]debug, A shape: torch.Size([32, 3, 200, 200])\n",
      "debug, P shape: torch.Size([32, 3, 200, 200])\n",
      "debug, N shape: torch.Size([32, 3, 200, 200])\n",
      "  0%|          | 0/200 [00:03<?, ?it/s]\n",
      "100%|██████████| 50/50 [00:48<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED_WEIGHTS_SUCCESS\n",
      "EPOCHS : 10 train_loss : 0.006700929403305054 valid_loss : 0.9863779938220978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]debug, A shape: torch.Size([32, 3, 200, 200])\n",
      "debug, P shape: torch.Size([32, 3, 200, 200])\n",
      "debug, N shape: torch.Size([32, 3, 200, 200])\n",
      "  0%|          | 0/200 [00:03<?, ?it/s]\n",
      "100%|██████████| 50/50 [00:48<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCHS : 11 train_loss : 0.007356112003326416 valid_loss : 0.9924172747135163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]debug, A shape: torch.Size([32, 3, 200, 200])\n",
      "debug, P shape: torch.Size([32, 3, 200, 200])\n",
      "debug, N shape: torch.Size([32, 3, 200, 200])\n",
      "  0%|          | 0/200 [00:04<?, ?it/s]\n",
      "100%|██████████| 50/50 [00:49<00:00,  1.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCHS : 12 train_loss : 0.006750366687774658 valid_loss : 1.035324090719223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]debug, A shape: torch.Size([32, 3, 200, 200])\n",
      "debug, P shape: torch.Size([32, 3, 200, 200])\n",
      "debug, N shape: torch.Size([32, 3, 200, 200])\n",
      "  0%|          | 0/200 [00:04<?, ?it/s]\n",
      "100%|██████████| 50/50 [00:49<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCHS : 13 train_loss : 0.006952395439147949 valid_loss : 1.072048316001892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]debug, A shape: torch.Size([32, 3, 200, 200])\n",
      "debug, P shape: torch.Size([32, 3, 200, 200])\n",
      "debug, N shape: torch.Size([32, 3, 200, 200])\n",
      "  0%|          | 0/200 [00:03<?, ?it/s]\n",
      "100%|██████████| 50/50 [00:50<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCHS : 14 train_loss : 0.006890549659729004 valid_loss : 1.1665906715393066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]debug, A shape: torch.Size([32, 3, 200, 200])\n",
      "debug, P shape: torch.Size([32, 3, 200, 200])\n",
      "debug, N shape: torch.Size([32, 3, 200, 200])\n",
      "  0%|          | 0/200 [00:03<?, ?it/s]\n",
      "100%|██████████| 50/50 [00:48<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCHS : 15 train_loss : 0.005532442927360535 valid_loss : 1.3246399009227752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]debug, A shape: torch.Size([32, 3, 200, 200])\n",
      "debug, P shape: torch.Size([32, 3, 200, 200])\n",
      "debug, N shape: torch.Size([32, 3, 200, 200])\n",
      "  0%|          | 0/200 [00:03<?, ?it/s]\n",
      "100%|██████████| 50/50 [00:47<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCHS : 16 train_loss : 0.008474782705307007 valid_loss : 1.7656272804737092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]debug, A shape: torch.Size([32, 3, 200, 200])\n",
      "debug, P shape: torch.Size([32, 3, 200, 200])\n",
      "debug, N shape: torch.Size([32, 3, 200, 200])\n",
      "  0%|          | 0/200 [00:03<?, ?it/s]\n",
      "100%|██████████| 50/50 [00:46<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCHS : 17 train_loss : 0.006771793365478515 valid_loss : 2.4584686398506164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]debug, A shape: torch.Size([32, 3, 200, 200])\n",
      "debug, P shape: torch.Size([32, 3, 200, 200])\n",
      "debug, N shape: torch.Size([32, 3, 200, 200])\n",
      "  0%|          | 0/200 [00:03<?, ?it/s]\n",
      "100%|██████████| 50/50 [00:46<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCHS : 18 train_loss : 0.0063381427526474 valid_loss : 2.59737242937088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]debug, A shape: torch.Size([32, 3, 200, 200])\n",
      "debug, P shape: torch.Size([32, 3, 200, 200])\n",
      "debug, N shape: torch.Size([32, 3, 200, 200])\n",
      "  0%|          | 0/200 [00:03<?, ?it/s]\n",
      "100%|██████████| 50/50 [00:46<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCHS : 19 train_loss : 0.010566526651382446 valid_loss : 2.378260307312012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]debug, A shape: torch.Size([32, 3, 200, 200])\n",
      "debug, P shape: torch.Size([32, 3, 200, 200])\n",
      "debug, N shape: torch.Size([32, 3, 200, 200])\n",
      "  0%|          | 0/200 [00:03<?, ?it/s]\n",
      "100%|██████████| 50/50 [00:46<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCHS : 20 train_loss : 0.008123047947883606 valid_loss : 2.353985582590103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]debug, A shape: torch.Size([32, 3, 200, 200])\n",
      "debug, P shape: torch.Size([32, 3, 200, 200])\n",
      "debug, N shape: torch.Size([32, 3, 200, 200])\n",
      "  0%|          | 0/200 [00:03<?, ?it/s]\n",
      "100%|██████████| 50/50 [00:46<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCHS : 21 train_loss : 0.0066129457950592045 valid_loss : 1.8781253612041473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]debug, A shape: torch.Size([32, 3, 200, 200])\n",
      "debug, P shape: torch.Size([32, 3, 200, 200])\n",
      "debug, N shape: torch.Size([32, 3, 200, 200])\n",
      "  0%|          | 0/200 [00:03<?, ?it/s]\n",
      "100%|██████████| 50/50 [01:16<00:00,  1.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCHS : 22 train_loss : 0.007654682993888855 valid_loss : 1.7799255955219269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]debug, A shape: torch.Size([32, 3, 200, 200])\n",
      "debug, P shape: torch.Size([32, 3, 200, 200])\n",
      "debug, N shape: torch.Size([32, 3, 200, 200])\n",
      "  0%|          | 0/200 [00:03<?, ?it/s]\n",
      "100%|██████████| 50/50 [00:44<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCHS : 23 train_loss : 0.0064543807506561275 valid_loss : 1.7491765820980072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]debug, A shape: torch.Size([32, 3, 200, 200])\n",
      "debug, P shape: torch.Size([32, 3, 200, 200])\n",
      "debug, N shape: torch.Size([32, 3, 200, 200])\n",
      "  0%|          | 0/200 [00:03<?, ?it/s]\n",
      "100%|██████████| 50/50 [00:48<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCHS : 24 train_loss : 0.004505143165588379 valid_loss : 1.682971991300583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]debug, A shape: torch.Size([32, 3, 200, 200])\n",
      "debug, P shape: torch.Size([32, 3, 200, 200])\n",
      "debug, N shape: torch.Size([32, 3, 200, 200])\n",
      "  0%|          | 0/200 [00:04<?, ?it/s]\n",
      "100%|██████████| 50/50 [00:52<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCHS : 25 train_loss : 0.005061110854148865 valid_loss : 1.6843982815742493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]debug, A shape: torch.Size([32, 3, 200, 200])\n",
      "debug, P shape: torch.Size([32, 3, 200, 200])\n",
      "debug, N shape: torch.Size([32, 3, 200, 200])\n",
      "  0%|          | 0/200 [00:04<?, ?it/s]\n",
      "100%|██████████| 50/50 [00:51<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCHS : 26 train_loss : 0.004183213710784912 valid_loss : 1.7017081665992737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]debug, A shape: torch.Size([32, 3, 200, 200])\n",
      "debug, P shape: torch.Size([32, 3, 200, 200])\n",
      "debug, N shape: torch.Size([32, 3, 200, 200])\n",
      "  0%|          | 0/200 [00:04<?, ?it/s]\n",
      "100%|██████████| 50/50 [00:51<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCHS : 27 train_loss : 0.008787307143211364 valid_loss : 1.5898219382762908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]debug, A shape: torch.Size([32, 3, 200, 200])\n",
      "debug, P shape: torch.Size([32, 3, 200, 200])\n",
      "debug, N shape: torch.Size([32, 3, 200, 200])\n",
      "  0%|          | 0/200 [00:03<?, ?it/s]\n",
      "100%|██████████| 50/50 [00:51<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCHS : 28 train_loss : 0.005406515002250672 valid_loss : 1.2682227039337157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]debug, A shape: torch.Size([32, 3, 200, 200])\n",
      "debug, P shape: torch.Size([32, 3, 200, 200])\n",
      "debug, N shape: torch.Size([32, 3, 200, 200])\n",
      "  0%|          | 0/200 [00:04<?, ?it/s]\n",
      "100%|██████████| 50/50 [00:50<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCHS : 29 train_loss : 0.005316870212554932 valid_loss : 1.2806662702560425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]debug, A shape: torch.Size([32, 3, 200, 200])\n",
      "debug, P shape: torch.Size([32, 3, 200, 200])\n",
      "debug, N shape: torch.Size([32, 3, 200, 200])\n",
      "  0%|          | 0/200 [00:04<?, ?it/s]\n",
      "100%|██████████| 50/50 [00:58<00:00,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCHS : 30 train_loss : 0.005211649537086487 valid_loss : 1.2873055493831635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "best_valid_loss = np.Inf\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "  train_loss = train_fn(model, trainloader, optimizer, criterion)\n",
    "  valid_loss = eval_fn(model, validloader, criterion)\n",
    "\n",
    "  if valid_loss < best_valid_loss:\n",
    "    torch.save(model.state_dict(), 'best_model.pt')\n",
    "    best_valid_loss = valid_loss\n",
    "    print(\"SAVED_WEIGHTS_SUCCESS\")\n",
    "\n",
    "  print(f\"EPOCHS : {i+1} train_loss : {train_loss} valid_loss : {valid_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# QUESTA E' LA FUNZIONE PER GENERARE I VETTORI DI ENCODING o embeddings\n",
    "def get_encoding_csv(model, anc_img_names, dirFolderReal, dirFolderFake):\n",
    "  anc_img_names_arr = np.array(anc_img_names)\n",
    "  encodings = []\n",
    "\n",
    "  model.eval()\n",
    "  \n",
    "  with torch.no_grad():\n",
    "    for i in tqdm(anc_img_names_arr):\n",
    "      if \"coco\" in i:\n",
    "        A = io.imread(os.path.join(dirFolderReal,i))\n",
    "        A = torch.from_numpy(A).permute(2, 0, 1) / 255.0\n",
    "        #A = np.expand_dims(A, 0)\n",
    "        #A = torch.from_numpy(A.astype(np.int32)) / 255.0\n",
    "        A = A.to(DEVICE)\n",
    "        A_enc = model(A.unsqueeze(0))\n",
    "        encodings.append(A_enc.squeeze().cpu().detach().numpy())\n",
    "      else:\n",
    "        A = io.imread(os.path.join(dirFolderFake,i))\n",
    "        A = torch.from_numpy(A).permute(2, 0, 1) / 255.0\n",
    "        #A = np.expand_dims(A, 0)\n",
    "        #A = torch.from_numpy(A.astype(np.int32)) / 255.0\n",
    "        A = A.to(DEVICE)\n",
    "        A_enc = model(A.unsqueeze(0))\n",
    "        encodings.append(A_enc.squeeze().cpu().detach().numpy())         \n",
    "\n",
    "  #with torch.no_grad():\n",
    "  #  for i in tqdm(anc_img_names_arr):\n",
    "  #    A = io.imread(os.path.join(dirFolderReal,i))\n",
    "  #    A = torch.from_numpy(A).permute(2, 0, 1) / 255.0\n",
    "  #    #A = np.expand_dims(A, 0)\n",
    "  #    #A = torch.from_numpy(A.astype(np.int32)) / 255.0\n",
    "  #    A = A.to(DEVICE)\n",
    "  #    A_enc = model(A.unsqueeze(0))\n",
    "  #    encodings.append(A_enc.squeeze().cpu().detach().numpy())\n",
    "\n",
    "    encodings = np.array(encodings)\n",
    "    encodings = pd.DataFrame(encodings)\n",
    "    df_enc = pd.concat([anc_img_names, encodings], axis = 1)\n",
    "\n",
    "    return df_enc"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-07T17:47:05.358268Z",
     "start_time": "2024-05-07T17:47:05.009948Z"
    }
   },
   "id": "d4711f9752b8a200",
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "087c6f6f-3d8d-437f-ab31-ce2c3a1c239c",
   "metadata": {
    "id": "087c6f6f-3d8d-437f-ab31-ce2c3a1c239c",
    "outputId": "10e29b3a-1d0f-41bb-e9a2-21aec49dac69",
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-05-07T17:56:15.244940Z",
     "start_time": "2024-05-07T17:47:05.361255Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8000/8000 [09:09<00:00, 14.56it/s]\n"
     ]
    }
   ],
   "source": [
    "# QUI RICARICO IL MODELLO UNA VOLTA TRAINATO\n",
    "model.load_state_dict(torch.load('best_model.pt'))\n",
    "\n",
    "# QUI CREO IL DATABASE DI FEATURE VECTORS DEL TRAINING SET\n",
    "# gli embeddings vengono aggiunti nel file csv per non rifarlo ad ogni allenamento\n",
    "df_enc = get_encoding_csv(model, df['Anchor'], real_data_dir, fake_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5a1096d8-a3dc-46bd-bb54-ed3debea3c57",
   "metadata": {
    "id": "5a1096d8-a3dc-46bd-bb54-ed3debea3c57",
    "outputId": "171dab62-2058-470c-9abf-5ea9495da9b0",
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-05-07T17:56:23.056118Z",
     "start_time": "2024-05-07T17:56:15.250943Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                  Anchor         0         1         2  \\\n0   coco/coco2017/test2017/img040098.jpg  0.159640  0.054152  0.364224   \n1    latentdiff-t2i/images/img009822.jpg  0.149158  0.057689  0.355407   \n2  coco/coco2017/train2017/img141504.jpg  0.174549  0.097916  0.356932   \n3   coco/coco2017/test2017/img001997.jpg  0.155925  0.058020  0.344176   \n4   coco/coco2017/test2017/img006486.jpg  0.153231  0.058798  0.360315   \n\n          3         4         5         6         7         8  ...       502  \\\n0 -0.201110 -0.104188 -0.126346  0.079230  0.289032 -0.309731  ...  0.210002   \n1 -0.197555 -0.084483 -0.134156  0.093355  0.286520 -0.307592  ...  0.220658   \n2 -0.236900 -0.064447 -0.092629  0.137812  0.321238 -0.272702  ...  0.221885   \n3 -0.201897 -0.112124 -0.147898  0.068810  0.290274 -0.289013  ...  0.204049   \n4 -0.201963 -0.080359 -0.126246  0.105663  0.304313 -0.305242  ...  0.230245   \n\n        503       504       505       506       507       508       509  \\\n0 -0.044827  0.141645  0.310006  0.102627 -0.080483 -0.240478  0.199235   \n1 -0.042755  0.131508  0.309401  0.100807 -0.065847 -0.234253  0.211387   \n2 -0.082382  0.153006  0.283265  0.116988 -0.042605 -0.251136  0.239386   \n3 -0.048589  0.150834  0.308076  0.103733 -0.066793 -0.222250  0.210142   \n4 -0.047763  0.131382  0.304406  0.111906 -0.066981 -0.247726  0.220985   \n\n        510       511  \n0  0.018837 -0.020826  \n1  0.030263 -0.049613  \n2  0.008668 -0.071179  \n3  0.048103 -0.028085  \n4  0.027719 -0.047383  \n\n[5 rows x 513 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Anchor</th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>...</th>\n      <th>502</th>\n      <th>503</th>\n      <th>504</th>\n      <th>505</th>\n      <th>506</th>\n      <th>507</th>\n      <th>508</th>\n      <th>509</th>\n      <th>510</th>\n      <th>511</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>coco/coco2017/test2017/img040098.jpg</td>\n      <td>0.159640</td>\n      <td>0.054152</td>\n      <td>0.364224</td>\n      <td>-0.201110</td>\n      <td>-0.104188</td>\n      <td>-0.126346</td>\n      <td>0.079230</td>\n      <td>0.289032</td>\n      <td>-0.309731</td>\n      <td>...</td>\n      <td>0.210002</td>\n      <td>-0.044827</td>\n      <td>0.141645</td>\n      <td>0.310006</td>\n      <td>0.102627</td>\n      <td>-0.080483</td>\n      <td>-0.240478</td>\n      <td>0.199235</td>\n      <td>0.018837</td>\n      <td>-0.020826</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>latentdiff-t2i/images/img009822.jpg</td>\n      <td>0.149158</td>\n      <td>0.057689</td>\n      <td>0.355407</td>\n      <td>-0.197555</td>\n      <td>-0.084483</td>\n      <td>-0.134156</td>\n      <td>0.093355</td>\n      <td>0.286520</td>\n      <td>-0.307592</td>\n      <td>...</td>\n      <td>0.220658</td>\n      <td>-0.042755</td>\n      <td>0.131508</td>\n      <td>0.309401</td>\n      <td>0.100807</td>\n      <td>-0.065847</td>\n      <td>-0.234253</td>\n      <td>0.211387</td>\n      <td>0.030263</td>\n      <td>-0.049613</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>coco/coco2017/train2017/img141504.jpg</td>\n      <td>0.174549</td>\n      <td>0.097916</td>\n      <td>0.356932</td>\n      <td>-0.236900</td>\n      <td>-0.064447</td>\n      <td>-0.092629</td>\n      <td>0.137812</td>\n      <td>0.321238</td>\n      <td>-0.272702</td>\n      <td>...</td>\n      <td>0.221885</td>\n      <td>-0.082382</td>\n      <td>0.153006</td>\n      <td>0.283265</td>\n      <td>0.116988</td>\n      <td>-0.042605</td>\n      <td>-0.251136</td>\n      <td>0.239386</td>\n      <td>0.008668</td>\n      <td>-0.071179</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>coco/coco2017/test2017/img001997.jpg</td>\n      <td>0.155925</td>\n      <td>0.058020</td>\n      <td>0.344176</td>\n      <td>-0.201897</td>\n      <td>-0.112124</td>\n      <td>-0.147898</td>\n      <td>0.068810</td>\n      <td>0.290274</td>\n      <td>-0.289013</td>\n      <td>...</td>\n      <td>0.204049</td>\n      <td>-0.048589</td>\n      <td>0.150834</td>\n      <td>0.308076</td>\n      <td>0.103733</td>\n      <td>-0.066793</td>\n      <td>-0.222250</td>\n      <td>0.210142</td>\n      <td>0.048103</td>\n      <td>-0.028085</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>coco/coco2017/test2017/img006486.jpg</td>\n      <td>0.153231</td>\n      <td>0.058798</td>\n      <td>0.360315</td>\n      <td>-0.201963</td>\n      <td>-0.080359</td>\n      <td>-0.126246</td>\n      <td>0.105663</td>\n      <td>0.304313</td>\n      <td>-0.305242</td>\n      <td>...</td>\n      <td>0.230245</td>\n      <td>-0.047763</td>\n      <td>0.131382</td>\n      <td>0.304406</td>\n      <td>0.111906</td>\n      <td>-0.066981</td>\n      <td>-0.247726</td>\n      <td>0.220985</td>\n      <td>0.027719</td>\n      <td>-0.047383</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 513 columns</p>\n</div>"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# QUI IL DATABASE COME CSV IN MODO TALE DA NON DOVER FARE QUESTA OPERAZIONE OGNI VOLTA\n",
    "# OVVIAMENTE, SE DEVO FARE UN NUOVO TRAINING DEVO ANCHE RICREARE GLI ENCODINGS\n",
    "df_enc.to_csv('database.csv', index = False)\n",
    "\n",
    "df_enc = pd.read_csv('database.csv')\n",
    "df_enc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ea4d49ef-7e4b-4a09-a188-d6afa1fc273d",
   "metadata": {
    "id": "ea4d49ef-7e4b-4a09-a188-d6afa1fc273d",
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-05-07T17:56:23.263131Z",
     "start_time": "2024-05-07T17:56:23.058130Z"
    }
   },
   "outputs": [],
   "source": [
    "# approssimazione della distanza, senza la radice quadrata, per fare i primi\n",
    "# allenamenti velocemente\n",
    "def euclidean_dist(img_enc, anc_enc_arr):\n",
    "    #dist = np.sqrt(np.dot(img_enc-anc_enc_arr, (img_enc- anc_enc_arr).T))\n",
    "    dist = np.dot(img_enc-anc_enc_arr, (img_enc- anc_enc_arr).T)\n",
    "    #dist = np.sqrt(dist)\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "38e0d751-6559-43b8-94f1-eab11f754bdd",
   "metadata": {
    "id": "38e0d751-6559-43b8-94f1-eab11f754bdd",
    "outputId": "7ff19abf-6ff7-4f31-bd3e-a07d07ca90dd",
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-05-07T17:56:23.564828Z",
     "start_time": "2024-05-07T17:56:23.267121Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       coco/coco2017/train2017/img059641.jpg\n",
      "1       coco/coco2017/train2017/img124043.jpg\n",
      "2       coco/coco2017/train2017/img045848.jpg\n",
      "3       coco/coco2017/train2017/img053090.jpg\n",
      "4        coco/coco2017/test2017/img001097.jpg\n",
      "                        ...                  \n",
      "1595     coco/coco2017/test2017/img026670.jpg\n",
      "1596     coco/coco2017/test2017/img020016.jpg\n",
      "1597     coco/coco2017/test2017/img039996.jpg\n",
      "1598    coco/coco2017/train2017/img084695.jpg\n",
      "1599    coco/coco2017/train2017/img049458.jpg\n",
      "Name: real, Length: 1600, dtype: object\n",
      "3200\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                    real                                 fake\n0  coco/coco2017/train2017/img059641.jpg         latentdiff/719/img006890.jpg\n1  coco/coco2017/train2017/img124043.jpg         latentdiff/321/img002486.jpg\n2  coco/coco2017/train2017/img045848.jpg  latentdiff-t2i/images/img007589.jpg\n3  coco/coco2017/train2017/img053090.jpg         latentdiff/565/img005180.jpg\n4   coco/coco2017/test2017/img001097.jpg          latentdiff/41/img003460.jpg",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>real</th>\n      <th>fake</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>coco/coco2017/train2017/img059641.jpg</td>\n      <td>latentdiff/719/img006890.jpg</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>coco/coco2017/train2017/img124043.jpg</td>\n      <td>latentdiff/321/img002486.jpg</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>coco/coco2017/train2017/img045848.jpg</td>\n      <td>latentdiff-t2i/images/img007589.jpg</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>coco/coco2017/train2017/img053090.jpg</td>\n      <td>latentdiff/565/img005180.jpg</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>coco/coco2017/test2017/img001097.jpg</td>\n      <td>latentdiff/41/img003460.jpg</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = os.path.join(Path(os.getcwd()).parent, 'datasets', 'testList.csv')\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "print(df['real'])\n",
    "print(df.size)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2262d0bc-a58d-4663-8515-bccbdf870608",
   "metadata": {
    "id": "2262d0bc-a58d-4663-8515-bccbdf870608",
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-05-07T17:56:23.896581Z",
     "start_time": "2024-05-07T17:56:23.570836Z"
    }
   },
   "outputs": [],
   "source": [
    "def getImageEmbeddings(img, model):\n",
    "\n",
    "    # img = np.expand_dims(img, 0)\n",
    "    # img = torch.from_numpy(img) / 255\n",
    "    img = torch.from_numpy(img).permute(2, 0, 1) / 255.0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        img = img.to(DEVICE)\n",
    "        img_enc = model(img.unsqueeze(0))\n",
    "        img_enc = img_enc.detach().cpu().numpy()\n",
    "        img_enc = np.array(img_enc)\n",
    "\n",
    "    return img_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0b4b5bd4-222b-44a3-b32d-2aa1f9b3d405",
   "metadata": {
    "id": "0b4b5bd4-222b-44a3-b32d-2aa1f9b3d405",
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-05-07T17:56:24.198915Z",
     "start_time": "2024-05-07T17:56:23.902577Z"
    }
   },
   "outputs": [],
   "source": [
    "def searchInDatabase(img_enc, database):\n",
    "    anc_enc_arr = database.iloc[:, 1:].to_numpy()\n",
    "    anc_img_names = database['Anchor']\n",
    "\n",
    "    distance = []\n",
    "    for i in range(anc_enc_arr.shape[0]):\n",
    "        dist = euclidean_dist(img_enc, anc_enc_arr[i : i+1, :])\n",
    "        distance = np.append(distance, dist)\n",
    "\n",
    "    closest_idx = np.argsort(distance)\n",
    "\n",
    "    return database['Anchor'][closest_idx[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ddd32725-5e2d-42a7-8998-4920f05ed143",
   "metadata": {
    "id": "ddd32725-5e2d-42a7-8998-4920f05ed143",
    "outputId": "888e6f94-a62a-46e1-cf29-d11664da20b7",
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-05-07T17:56:24.484637Z",
     "start_time": "2024-05-07T17:56:24.202905Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(1600, 2)"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DataTestReal = 'C:/Users/polsi/Desktop/Lavori/DeepFake/Datasets/Artifact/cycle_gan/st/test/'\n",
    "path = Path(os.getcwd()).parent.parent\n",
    "real_data_dir = os.path.join(path, \"artifact\", \"coco\")\n",
    "fake_data_dir = os.path.join(path, \"artifact\", \"latent_diffusion\")\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "tempDf = df\n",
    "tempDf.head()\n",
    "tempDf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ca7392f6-109f-4a96-92b1-e85600be8d8a",
   "metadata": {
    "id": "ca7392f6-109f-4a96-92b1-e85600be8d8a",
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-05-07T18:03:15.497784Z",
     "start_time": "2024-05-07T17:56:24.488628Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1600it [06:50,  3.90it/s]\n"
     ]
    }
   ],
   "source": [
    "# testo i fake\n",
    "currentTest = 'fake'\n",
    "database = df_enc\n",
    "# prendo i primi 500 Fake\n",
    "for index, row in tqdm(tempDf.iterrows()):\n",
    "    path = os.path.join(fake_data_dir, row[currentTest])\n",
    "    img_name = path\n",
    "\n",
    "    img = io.imread(img_name)\n",
    "\n",
    "    img_enc = getImageEmbeddings(img, model)\n",
    "\n",
    "    closestLabel = searchInDatabase(img_enc, database)\n",
    "\n",
    "    if \"coco\" in closestLabel:\n",
    "        y_pred.append(\"real\")\n",
    "    else:\n",
    "        y_pred.append(\"fake\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fe3af5ae-b8c2-419e-b5a5-9d17609797f5",
   "metadata": {
    "id": "fe3af5ae-b8c2-419e-b5a5-9d17609797f5",
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-05-07T18:03:15.673892Z",
     "start_time": "2024-05-07T18:03:15.499784Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1600\n",
      "['real', 'real', 'real', 'real', 'fake', 'real', 'fake', 'fake', 'real', 'real', 'fake', 'fake', 'fake', 'real', 'real', 'fake', 'fake', 'real', 'fake', 'fake', 'fake', 'fake', 'real', 'real', 'fake', 'real', 'fake', 'fake', 'fake', 'fake', 'real', 'real', 'fake', 'fake', 'fake', 'real', 'real', 'fake', 'fake', 'real', 'fake', 'fake', 'fake', 'real', 'real', 'fake', 'fake', 'real', 'real', 'real', 'real', 'fake', 'fake', 'real', 'fake', 'fake', 'fake', 'fake', 'fake', 'real', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'real', 'real', 'fake', 'real', 'real', 'fake', 'real', 'real', 'real', 'fake', 'real', 'fake', 'real', 'real', 'real', 'fake', 'fake', 'real', 'real', 'real', 'fake', 'fake', 'real', 'fake', 'fake', 'fake', 'fake', 'real', 'real', 'real', 'real', 'real', 'fake', 'fake', 'real', 'real', 'real', 'real', 'real', 'fake', 'real', 'real', 'fake', 'real', 'fake', 'fake', 'fake', 'real', 'fake', 'real', 'fake', 'real', 'real', 'fake', 'real', 'fake', 'real', 'real', 'fake', 'real', 'fake', 'real', 'real', 'fake', 'fake', 'real', 'fake', 'real', 'fake', 'real', 'real', 'fake', 'fake', 'real', 'fake', 'real', 'fake', 'real', 'fake', 'fake', 'real', 'fake', 'real', 'real', 'fake', 'fake', 'real', 'real', 'real', 'fake', 'fake', 'fake', 'real', 'real', 'fake', 'fake', 'real', 'real', 'real', 'fake', 'fake', 'fake', 'fake', 'fake', 'real', 'fake', 'fake', 'fake', 'fake', 'fake', 'real', 'real', 'fake', 'real', 'real', 'fake', 'fake', 'real', 'fake', 'real', 'real', 'fake', 'real', 'real', 'fake', 'real', 'real', 'fake', 'fake', 'real', 'fake', 'fake', 'real', 'real', 'fake', 'real', 'real', 'real', 'fake', 'real', 'real', 'real', 'fake', 'fake', 'fake', 'real', 'real', 'fake', 'real', 'real', 'real', 'real', 'fake', 'fake', 'fake', 'fake', 'fake', 'real', 'fake', 'fake', 'real', 'real', 'real', 'fake', 'real', 'fake', 'fake', 'real', 'real', 'fake', 'fake', 'real', 'real', 'fake', 'fake', 'fake', 'real', 'real', 'fake', 'real', 'real', 'fake', 'real', 'fake', 'fake', 'fake', 'real', 'real', 'real', 'real', 'fake', 'fake', 'real', 'fake', 'real', 'fake', 'real', 'real', 'real', 'real', 'real', 'real', 'fake', 'fake', 'real', 'fake', 'fake', 'fake', 'real', 'fake', 'real', 'fake', 'real', 'fake', 'fake', 'fake', 'fake', 'real', 'fake', 'fake', 'fake', 'fake', 'real', 'fake', 'real', 'real', 'fake', 'fake', 'real', 'fake', 'fake', 'fake', 'real', 'real', 'real', 'real', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'real', 'fake', 'real', 'fake', 'real', 'fake', 'real', 'real', 'real', 'real', 'real', 'real', 'fake', 'fake', 'real', 'real', 'real', 'fake', 'real', 'real', 'fake', 'fake', 'real', 'real', 'fake', 'real', 'fake', 'real', 'fake', 'real', 'real', 'real', 'fake', 'fake', 'real', 'real', 'real', 'fake', 'real', 'real', 'real', 'real', 'fake', 'real', 'fake', 'real', 'real', 'real', 'real', 'real', 'real', 'real', 'fake', 'real', 'real', 'fake', 'real', 'fake', 'real', 'fake', 'fake', 'fake', 'real', 'real', 'fake', 'fake', 'fake', 'real', 'fake', 'fake', 'fake', 'real', 'fake', 'real', 'real', 'fake', 'real', 'real', 'fake', 'real', 'real', 'real', 'real', 'real', 'real', 'real', 'fake', 'fake', 'real', 'fake', 'real', 'fake', 'fake', 'real', 'real', 'fake', 'fake', 'real', 'fake', 'real', 'fake', 'real', 'real', 'real', 'real', 'real', 'fake', 'fake', 'real', 'fake', 'fake', 'real', 'fake', 'real', 'fake', 'real', 'real', 'fake', 'real', 'real', 'fake', 'real', 'real', 'fake', 'fake', 'real', 'fake', 'real', 'fake', 'fake', 'fake', 'fake', 'real', 'fake', 'real', 'fake', 'real', 'fake', 'real', 'real', 'fake', 'real', 'fake', 'real', 'real', 'fake', 'fake', 'fake', 'real', 'real', 'fake', 'real', 'real', 'fake', 'fake', 'real', 'real', 'real', 'real', 'fake', 'fake', 'fake', 'real', 'real', 'fake', 'fake', 'fake', 'real', 'fake', 'real', 'fake', 'fake', 'real', 'fake', 'fake', 'fake', 'real', 'real', 'real', 'fake', 'fake', 'real', 'real', 'fake', 'real', 'real', 'real', 'fake', 'real', 'real', 'fake', 'fake', 'real', 'fake', 'fake', 'fake', 'fake', 'real', 'fake', 'fake', 'fake', 'fake', 'real', 'real', 'real', 'fake', 'real', 'fake', 'fake', 'fake', 'real', 'fake', 'fake', 'fake', 'fake', 'real', 'real', 'fake', 'fake', 'fake', 'real', 'real', 'real', 'real', 'fake', 'fake', 'real', 'fake', 'real', 'real', 'real', 'real', 'fake', 'real', 'real', 'real', 'fake', 'real', 'fake', 'fake', 'real', 'real', 'fake', 'real', 'fake', 'real', 'real', 'real', 'fake', 'real', 'real', 'fake', 'fake', 'fake', 'fake', 'fake', 'real', 'fake', 'fake', 'fake', 'fake', 'real', 'real', 'real', 'fake', 'fake', 'fake', 'real', 'real', 'fake', 'fake', 'fake', 'fake', 'fake', 'real', 'fake', 'fake', 'fake', 'fake', 'fake', 'real', 'fake', 'fake', 'fake', 'fake', 'fake', 'real', 'real', 'fake', 'fake', 'real', 'real', 'real', 'real', 'real', 'fake', 'fake', 'fake', 'real', 'real', 'real', 'real', 'fake', 'fake', 'real', 'real', 'real', 'real', 'fake', 'fake', 'real', 'fake', 'real', 'real', 'fake', 'real', 'real', 'real', 'fake', 'fake', 'real', 'fake', 'fake', 'real', 'real', 'fake', 'fake', 'fake', 'fake', 'real', 'fake', 'real', 'real', 'real', 'fake', 'real', 'real', 'real', 'fake', 'real', 'real', 'fake', 'real', 'real', 'real', 'fake', 'real', 'fake', 'fake', 'fake', 'real', 'real', 'real', 'fake', 'real', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'real', 'fake', 'real', 'fake', 'fake', 'fake', 'real', 'fake', 'fake', 'fake', 'real', 'real', 'real', 'real', 'real', 'real', 'real', 'fake', 'fake', 'real', 'real', 'real', 'fake', 'fake', 'real', 'real', 'real', 'fake', 'fake', 'real', 'real', 'real', 'real', 'real', 'fake', 'fake', 'real', 'fake', 'real', 'real', 'real', 'fake', 'real', 'real', 'real', 'fake', 'real', 'real', 'fake', 'fake', 'real', 'fake', 'fake', 'real', 'real', 'fake', 'real', 'real', 'real', 'real', 'fake', 'fake', 'real', 'real', 'real', 'fake', 'real', 'fake', 'real', 'fake', 'fake', 'fake', 'real', 'real', 'real', 'fake', 'fake', 'real', 'real', 'real', 'fake', 'real', 'real', 'fake', 'real', 'fake', 'real', 'fake', 'real', 'fake', 'fake', 'real', 'fake', 'real', 'fake', 'fake', 'real', 'fake', 'fake', 'fake', 'fake', 'real', 'fake', 'real', 'real', 'fake', 'real', 'real', 'fake', 'real', 'fake', 'real', 'fake', 'real', 'real', 'fake', 'fake', 'fake', 'real', 'fake', 'fake', 'real', 'real', 'real', 'real', 'real', 'fake', 'fake', 'fake', 'real', 'fake', 'real', 'fake', 'real', 'real', 'real', 'real', 'real', 'real', 'fake', 'real', 'real', 'fake', 'real', 'fake', 'real', 'real', 'fake', 'real', 'fake', 'real', 'real', 'fake', 'fake', 'fake', 'real', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'real', 'real', 'real', 'fake', 'fake', 'real', 'real', 'real', 'fake', 'fake', 'real', 'real', 'real', 'real', 'fake', 'real', 'fake', 'real', 'real', 'fake', 'fake', 'fake', 'real', 'fake', 'real', 'fake', 'real', 'real', 'fake', 'real', 'fake', 'fake', 'real', 'real', 'fake', 'fake', 'real', 'real', 'real', 'real', 'fake', 'real', 'real', 'real', 'real', 'real', 'real', 'fake', 'real', 'fake', 'fake', 'real', 'real', 'fake', 'fake', 'real', 'fake', 'real', 'real', 'real', 'real', 'fake', 'real', 'real', 'fake', 'real', 'fake', 'fake', 'real', 'real', 'fake', 'fake', 'real', 'fake', 'real', 'real', 'fake', 'fake', 'real', 'real', 'real', 'fake', 'fake', 'real', 'fake', 'real', 'fake', 'fake', 'real', 'real', 'real', 'fake', 'fake', 'fake', 'fake', 'real', 'real', 'fake', 'real', 'real', 'real', 'real', 'real', 'real', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'real', 'fake', 'real', 'fake', 'fake', 'real', 'fake', 'real', 'fake', 'fake', 'real', 'real', 'fake', 'real', 'real', 'fake', 'fake', 'fake', 'real', 'fake', 'real', 'fake', 'real', 'real', 'fake', 'real', 'real', 'real', 'fake', 'fake', 'real', 'fake', 'fake', 'fake', 'fake', 'real', 'real', 'real', 'real', 'fake', 'fake', 'fake', 'real', 'fake', 'real', 'real', 'fake', 'real', 'real', 'fake', 'real', 'real', 'fake', 'fake', 'real', 'fake', 'fake', 'real', 'fake', 'fake', 'fake', 'real', 'real', 'real', 'fake', 'real', 'fake', 'real', 'fake', 'fake', 'fake', 'fake', 'real', 'real', 'fake', 'fake', 'real', 'fake', 'fake', 'real', 'fake', 'real', 'real', 'real', 'real', 'real', 'real', 'fake', 'real', 'real', 'fake', 'real', 'fake', 'real', 'fake', 'real', 'fake', 'real', 'real', 'fake', 'fake', 'fake', 'real', 'fake', 'real', 'fake', 'fake', 'real', 'fake', 'real', 'fake', 'fake', 'fake', 'fake', 'real', 'real', 'real', 'fake', 'fake', 'real', 'real', 'real', 'fake', 'real', 'fake', 'real', 'real', 'fake', 'fake', 'real', 'real', 'fake', 'real', 'real', 'fake', 'fake', 'fake', 'real', 'real', 'fake', 'real', 'fake', 'fake', 'real', 'fake', 'fake', 'real', 'fake', 'fake', 'fake', 'real', 'real', 'real', 'fake', 'real', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'real', 'fake', 'fake', 'real', 'fake', 'real', 'fake', 'fake', 'fake', 'fake', 'fake', 'real', 'fake', 'fake', 'real', 'real', 'fake', 'fake', 'real', 'real', 'real', 'fake', 'fake', 'fake', 'real', 'real', 'real', 'fake', 'fake', 'fake', 'fake', 'fake', 'real', 'real', 'fake', 'real', 'real', 'fake', 'real', 'real', 'real', 'real', 'real', 'real', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'real', 'fake', 'real', 'real', 'fake', 'real', 'real', 'real', 'fake', 'fake', 'real', 'real', 'real', 'real', 'real', 'fake', 'fake', 'real', 'real', 'real', 'real', 'real', 'real', 'fake', 'real', 'real', 'real', 'fake', 'fake', 'real', 'real', 'fake', 'real', 'real', 'fake', 'real', 'real', 'fake', 'fake', 'fake', 'real', 'real', 'real', 'fake', 'fake', 'real', 'fake', 'fake', 'real', 'real', 'fake', 'fake', 'fake', 'real', 'real', 'real', 'real', 'real', 'fake', 'real', 'real', 'fake', 'fake', 'fake', 'real', 'fake', 'real', 'real', 'real', 'fake', 'real', 'real', 'fake', 'real', 'fake', 'real', 'real', 'fake', 'fake', 'fake', 'fake', 'real', 'real', 'fake', 'real', 'fake', 'real', 'real', 'fake', 'real', 'fake', 'real', 'fake', 'fake', 'fake', 'fake', 'real', 'fake', 'fake', 'real', 'fake', 'fake', 'real', 'real', 'fake', 'real', 'fake', 'real', 'real', 'real', 'fake', 'real', 'real', 'real', 'real', 'fake', 'fake', 'real', 'fake', 'real', 'fake', 'fake', 'real', 'fake', 'fake', 'fake', 'fake', 'real', 'real', 'real', 'fake', 'fake', 'real', 'real', 'real', 'real', 'fake', 'real', 'real', 'real', 'fake', 'real', 'real', 'fake', 'real', 'real', 'real', 'real', 'real', 'fake', 'real', 'fake', 'real', 'fake', 'fake', 'fake', 'fake', 'fake', 'real', 'fake', 'fake', 'fake', 'real', 'fake', 'real', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'real', 'real', 'real', 'fake', 'fake', 'real', 'real', 'real', 'real', 'fake', 'real', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'real', 'real', 'real', 'real', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'real', 'fake', 'real', 'fake', 'real', 'fake', 'fake', 'real', 'fake', 'fake', 'fake', 'fake', 'fake', 'real', 'fake', 'real', 'real', 'real', 'fake', 'real', 'fake', 'fake', 'real', 'fake', 'fake', 'real', 'fake', 'real', 'real', 'fake', 'fake', 'fake', 'fake', 'fake', 'real', 'fake', 'real', 'real', 'fake', 'real', 'real', 'fake', 'real', 'fake', 'fake', 'real', 'real', 'fake', 'real', 'real', 'fake', 'fake', 'fake', 'fake', 'real', 'real', 'fake', 'fake', 'real', 'real', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'real', 'fake', 'real', 'real', 'fake', 'real', 'fake', 'fake', 'real', 'fake', 'fake', 'real', 'real', 'fake', 'real', 'fake', 'real', 'fake', 'real', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'real', 'fake', 'real', 'real', 'real', 'real', 'real', 'fake', 'real', 'fake', 'real', 'real', 'fake', 'real', 'fake', 'real', 'real', 'fake', 'real', 'real', 'fake', 'fake', 'fake', 'real', 'real', 'real', 'fake', 'fake', 'fake', 'fake', 'real', 'real', 'fake', 'real', 'fake', 'fake', 'fake', 'real', 'real', 'fake', 'fake', 'fake', 'fake', 'real', 'fake', 'fake', 'real', 'real', 'real', 'fake', 'real', 'fake', 'fake', 'fake', 'fake', 'real', 'fake', 'real', 'real', 'fake', 'fake', 'fake', 'real', 'real', 'fake', 'real', 'real', 'fake', 'fake', 'fake', 'real', 'real', 'real', 'fake', 'real', 'real', 'real', 'fake', 'fake', 'fake', 'real', 'fake', 'fake', 'real', 'fake', 'fake', 'real', 'real', 'real', 'fake', 'real', 'real', 'fake', 'real', 'real', 'real', 'real', 'real', 'real', 'fake', 'fake', 'real', 'real', 'real', 'fake', 'real', 'real', 'real', 'fake', 'fake', 'real', 'fake', 'real', 'fake', 'fake', 'real', 'fake', 'fake', 'real', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'real', 'real', 'fake', 'real', 'real', 'fake', 'real', 'fake', 'real', 'real', 'real', 'fake', 'real', 'real', 'fake', 'fake', 'fake', 'real', 'real', 'fake', 'real', 'fake', 'fake', 'fake', 'real', 'real', 'fake', 'real', 'fake', 'fake']\n"
     ]
    }
   ],
   "source": [
    "print(len(y_true))\n",
    "print(len(y_pred))\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0e5e4b6e-82f4-4d4f-b797-d8e24be8ec3e",
   "metadata": {
    "id": "0e5e4b6e-82f4-4d4f-b797-d8e24be8ec3e",
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-05-07T18:03:15.861875Z",
     "start_time": "2024-05-07T18:03:15.675786Z"
    }
   },
   "outputs": [],
   "source": [
    "database = df_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "46b8f2c1-a3ca-4bbd-8326-02685fc44cf8",
   "metadata": {
    "id": "46b8f2c1-a3ca-4bbd-8326-02685fc44cf8",
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-05-07T18:08:26.383353Z",
     "start_time": "2024-05-07T18:03:15.863861Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1600it [05:10,  5.16it/s]\n"
     ]
    }
   ],
   "source": [
    "# testo i real\n",
    "currentTest = 'real'\n",
    "# prendo i primi 500 Fake\n",
    "for index, row in tqdm(tempDf.iterrows()):\n",
    "    path = os.path.join(real_data_dir, row[currentTest])\n",
    "    img_name = path\n",
    "\n",
    "    img_enc = getImageEmbeddings(img, model)\n",
    "\n",
    "    closestLabel = searchInDatabase(img_enc, database)\n",
    "    if \"coco\" in closestLabel:\n",
    "        y_pred.append(\"real\")\n",
    "    else:\n",
    "        y_pred.append(\"fake\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4c465bfd-18ad-4750-b689-739b712185ab",
   "metadata": {
    "id": "4c465bfd-18ad-4750-b689-739b712185ab",
    "outputId": "e974c712-91fb-4fae-c589-85e08a50fb77",
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-05-07T18:08:26.495354Z",
     "start_time": "2024-05-07T18:08:26.385354Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "3200\n",
      "['real', 'real', 'real', 'real', 'fake', 'real', 'fake', 'fake', 'real', 'real', 'fake', 'fake', 'fake', 'real', 'real', 'fake', 'fake', 'real', 'fake', 'fake', 'fake', 'fake', 'real', 'real', 'fake', 'real', 'fake', 'fake', 'fake', 'fake', 'real', 'real', 'fake', 'fake', 'fake', 'real', 'real', 'fake', 'fake', 'real', 'fake', 'fake', 'fake', 'real', 'real', 'fake', 'fake', 'real', 'real', 'real', 'real', 'fake', 'fake', 'real', 'fake', 'fake', 'fake', 'fake', 'fake', 'real', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'real', 'real', 'fake', 'real', 'real', 'fake', 'real', 'real', 'real', 'fake', 'real', 'fake', 'real', 'real', 'real', 'fake', 'fake', 'real', 'real', 'real', 'fake', 'fake', 'real', 'fake', 'fake', 'fake', 'fake', 'real', 'real', 'real', 'real', 'real', 'fake', 'fake', 'real', 'real', 'real', 'real', 'real', 'fake', 'real', 'real', 'fake', 'real', 'fake', 'fake', 'fake', 'real', 'fake', 'real', 'fake', 'real', 'real', 'fake', 'real', 'fake', 'real', 'real', 'fake', 'real', 'fake', 'real', 'real', 'fake', 'fake', 'real', 'fake', 'real', 'fake', 'real', 'real', 'fake', 'fake', 'real', 'fake', 'real', 'fake', 'real', 'fake', 'fake', 'real', 'fake', 'real', 'real', 'fake', 'fake', 'real', 'real', 'real', 'fake', 'fake', 'fake', 'real', 'real', 'fake', 'fake', 'real', 'real', 'real', 'fake', 'fake', 'fake', 'fake', 'fake', 'real', 'fake', 'fake', 'fake', 'fake', 'fake', 'real', 'real', 'fake', 'real', 'real', 'fake', 'fake', 'real', 'fake', 'real', 'real', 'fake', 'real', 'real', 'fake', 'real', 'real', 'fake', 'fake', 'real', 'fake', 'fake', 'real', 'real', 'fake', 'real', 'real', 'real', 'fake', 'real', 'real', 'real', 'fake', 'fake', 'fake', 'real', 'real', 'fake', 'real', 'real', 'real', 'real', 'fake', 'fake', 'fake', 'fake', 'fake', 'real', 'fake', 'fake', 'real', 'real', 'real', 'fake', 'real', 'fake', 'fake', 'real', 'real', 'fake', 'fake', 'real', 'real', 'fake', 'fake', 'fake', 'real', 'real', 'fake', 'real', 'real', 'fake', 'real', 'fake', 'fake', 'fake', 'real', 'real', 'real', 'real', 'fake', 'fake', 'real', 'fake', 'real', 'fake', 'real', 'real', 'real', 'real', 'real', 'real', 'fake', 'fake', 'real', 'fake', 'fake', 'fake', 'real', 'fake', 'real', 'fake', 'real', 'fake', 'fake', 'fake', 'fake', 'real', 'fake', 'fake', 'fake', 'fake', 'real', 'fake', 'real', 'real', 'fake', 'fake', 'real', 'fake', 'fake', 'fake', 'real', 'real', 'real', 'real', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'real', 'fake', 'real', 'fake', 'real', 'fake', 'real', 'real', 'real', 'real', 'real', 'real', 'fake', 'fake', 'real', 'real', 'real', 'fake', 'real', 'real', 'fake', 'fake', 'real', 'real', 'fake', 'real', 'fake', 'real', 'fake', 'real', 'real', 'real', 'fake', 'fake', 'real', 'real', 'real', 'fake', 'real', 'real', 'real', 'real', 'fake', 'real', 'fake', 'real', 'real', 'real', 'real', 'real', 'real', 'real', 'fake', 'real', 'real', 'fake', 'real', 'fake', 'real', 'fake', 'fake', 'fake', 'real', 'real', 'fake', 'fake', 'fake', 'real', 'fake', 'fake', 'fake', 'real', 'fake', 'real', 'real', 'fake', 'real', 'real', 'fake', 'real', 'real', 'real', 'real', 'real', 'real', 'real', 'fake', 'fake', 'real', 'fake', 'real', 'fake', 'fake', 'real', 'real', 'fake', 'fake', 'real', 'fake', 'real', 'fake', 'real', 'real', 'real', 'real', 'real', 'fake', 'fake', 'real', 'fake', 'fake', 'real', 'fake', 'real', 'fake', 'real', 'real', 'fake', 'real', 'real', 'fake', 'real', 'real', 'fake', 'fake', 'real', 'fake', 'real', 'fake', 'fake', 'fake', 'fake', 'real', 'fake', 'real', 'fake', 'real', 'fake', 'real', 'real', 'fake', 'real', 'fake', 'real', 'real', 'fake', 'fake', 'fake', 'real', 'real', 'fake', 'real', 'real', 'fake', 'fake', 'real', 'real', 'real', 'real', 'fake', 'fake', 'fake', 'real', 'real', 'fake', 'fake', 'fake', 'real', 'fake', 'real', 'fake', 'fake', 'real', 'fake', 'fake', 'fake', 'real', 'real', 'real', 'fake', 'fake', 'real', 'real', 'fake', 'real', 'real', 'real', 'fake', 'real', 'real', 'fake', 'fake', 'real', 'fake', 'fake', 'fake', 'fake', 'real', 'fake', 'fake', 'fake', 'fake', 'real', 'real', 'real', 'fake', 'real', 'fake', 'fake', 'fake', 'real', 'fake', 'fake', 'fake', 'fake', 'real', 'real', 'fake', 'fake', 'fake', 'real', 'real', 'real', 'real', 'fake', 'fake', 'real', 'fake', 'real', 'real', 'real', 'real', 'fake', 'real', 'real', 'real', 'fake', 'real', 'fake', 'fake', 'real', 'real', 'fake', 'real', 'fake', 'real', 'real', 'real', 'fake', 'real', 'real', 'fake', 'fake', 'fake', 'fake', 'fake', 'real', 'fake', 'fake', 'fake', 'fake', 'real', 'real', 'real', 'fake', 'fake', 'fake', 'real', 'real', 'fake', 'fake', 'fake', 'fake', 'fake', 'real', 'fake', 'fake', 'fake', 'fake', 'fake', 'real', 'fake', 'fake', 'fake', 'fake', 'fake', 'real', 'real', 'fake', 'fake', 'real', 'real', 'real', 'real', 'real', 'fake', 'fake', 'fake', 'real', 'real', 'real', 'real', 'fake', 'fake', 'real', 'real', 'real', 'real', 'fake', 'fake', 'real', 'fake', 'real', 'real', 'fake', 'real', 'real', 'real', 'fake', 'fake', 'real', 'fake', 'fake', 'real', 'real', 'fake', 'fake', 'fake', 'fake', 'real', 'fake', 'real', 'real', 'real', 'fake', 'real', 'real', 'real', 'fake', 'real', 'real', 'fake', 'real', 'real', 'real', 'fake', 'real', 'fake', 'fake', 'fake', 'real', 'real', 'real', 'fake', 'real', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'real', 'fake', 'real', 'fake', 'fake', 'fake', 'real', 'fake', 'fake', 'fake', 'real', 'real', 'real', 'real', 'real', 'real', 'real', 'fake', 'fake', 'real', 'real', 'real', 'fake', 'fake', 'real', 'real', 'real', 'fake', 'fake', 'real', 'real', 'real', 'real', 'real', 'fake', 'fake', 'real', 'fake', 'real', 'real', 'real', 'fake', 'real', 'real', 'real', 'fake', 'real', 'real', 'fake', 'fake', 'real', 'fake', 'fake', 'real', 'real', 'fake', 'real', 'real', 'real', 'real', 'fake', 'fake', 'real', 'real', 'real', 'fake', 'real', 'fake', 'real', 'fake', 'fake', 'fake', 'real', 'real', 'real', 'fake', 'fake', 'real', 'real', 'real', 'fake', 'real', 'real', 'fake', 'real', 'fake', 'real', 'fake', 'real', 'fake', 'fake', 'real', 'fake', 'real', 'fake', 'fake', 'real', 'fake', 'fake', 'fake', 'fake', 'real', 'fake', 'real', 'real', 'fake', 'real', 'real', 'fake', 'real', 'fake', 'real', 'fake', 'real', 'real', 'fake', 'fake', 'fake', 'real', 'fake', 'fake', 'real', 'real', 'real', 'real', 'real', 'fake', 'fake', 'fake', 'real', 'fake', 'real', 'fake', 'real', 'real', 'real', 'real', 'real', 'real', 'fake', 'real', 'real', 'fake', 'real', 'fake', 'real', 'real', 'fake', 'real', 'fake', 'real', 'real', 'fake', 'fake', 'fake', 'real', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'real', 'real', 'real', 'fake', 'fake', 'real', 'real', 'real', 'fake', 'fake', 'real', 'real', 'real', 'real', 'fake', 'real', 'fake', 'real', 'real', 'fake', 'fake', 'fake', 'real', 'fake', 'real', 'fake', 'real', 'real', 'fake', 'real', 'fake', 'fake', 'real', 'real', 'fake', 'fake', 'real', 'real', 'real', 'real', 'fake', 'real', 'real', 'real', 'real', 'real', 'real', 'fake', 'real', 'fake', 'fake', 'real', 'real', 'fake', 'fake', 'real', 'fake', 'real', 'real', 'real', 'real', 'fake', 'real', 'real', 'fake', 'real', 'fake', 'fake', 'real', 'real', 'fake', 'fake', 'real', 'fake', 'real', 'real', 'fake', 'fake', 'real', 'real', 'real', 'fake', 'fake', 'real', 'fake', 'real', 'fake', 'fake', 'real', 'real', 'real', 'fake', 'fake', 'fake', 'fake', 'real', 'real', 'fake', 'real', 'real', 'real', 'real', 'real', 'real', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'real', 'fake', 'real', 'fake', 'fake', 'real', 'fake', 'real', 'fake', 'fake', 'real', 'real', 'fake', 'real', 'real', 'fake', 'fake', 'fake', 'real', 'fake', 'real', 'fake', 'real', 'real', 'fake', 'real', 'real', 'real', 'fake', 'fake', 'real', 'fake', 'fake', 'fake', 'fake', 'real', 'real', 'real', 'real', 'fake', 'fake', 'fake', 'real', 'fake', 'real', 'real', 'fake', 'real', 'real', 'fake', 'real', 'real', 'fake', 'fake', 'real', 'fake', 'fake', 'real', 'fake', 'fake', 'fake', 'real', 'real', 'real', 'fake', 'real', 'fake', 'real', 'fake', 'fake', 'fake', 'fake', 'real', 'real', 'fake', 'fake', 'real', 'fake', 'fake', 'real', 'fake', 'real', 'real', 'real', 'real', 'real', 'real', 'fake', 'real', 'real', 'fake', 'real', 'fake', 'real', 'fake', 'real', 'fake', 'real', 'real', 'fake', 'fake', 'fake', 'real', 'fake', 'real', 'fake', 'fake', 'real', 'fake', 'real', 'fake', 'fake', 'fake', 'fake', 'real', 'real', 'real', 'fake', 'fake', 'real', 'real', 'real', 'fake', 'real', 'fake', 'real', 'real', 'fake', 'fake', 'real', 'real', 'fake', 'real', 'real', 'fake', 'fake', 'fake', 'real', 'real', 'fake', 'real', 'fake', 'fake', 'real', 'fake', 'fake', 'real', 'fake', 'fake', 'fake', 'real', 'real', 'real', 'fake', 'real', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'real', 'fake', 'fake', 'real', 'fake', 'real', 'fake', 'fake', 'fake', 'fake', 'fake', 'real', 'fake', 'fake', 'real', 'real', 'fake', 'fake', 'real', 'real', 'real', 'fake', 'fake', 'fake', 'real', 'real', 'real', 'fake', 'fake', 'fake', 'fake', 'fake', 'real', 'real', 'fake', 'real', 'real', 'fake', 'real', 'real', 'real', 'real', 'real', 'real', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'real', 'fake', 'real', 'real', 'fake', 'real', 'real', 'real', 'fake', 'fake', 'real', 'real', 'real', 'real', 'real', 'fake', 'fake', 'real', 'real', 'real', 'real', 'real', 'real', 'fake', 'real', 'real', 'real', 'fake', 'fake', 'real', 'real', 'fake', 'real', 'real', 'fake', 'real', 'real', 'fake', 'fake', 'fake', 'real', 'real', 'real', 'fake', 'fake', 'real', 'fake', 'fake', 'real', 'real', 'fake', 'fake', 'fake', 'real', 'real', 'real', 'real', 'real', 'fake', 'real', 'real', 'fake', 'fake', 'fake', 'real', 'fake', 'real', 'real', 'real', 'fake', 'real', 'real', 'fake', 'real', 'fake', 'real', 'real', 'fake', 'fake', 'fake', 'fake', 'real', 'real', 'fake', 'real', 'fake', 'real', 'real', 'fake', 'real', 'fake', 'real', 'fake', 'fake', 'fake', 'fake', 'real', 'fake', 'fake', 'real', 'fake', 'fake', 'real', 'real', 'fake', 'real', 'fake', 'real', 'real', 'real', 'fake', 'real', 'real', 'real', 'real', 'fake', 'fake', 'real', 'fake', 'real', 'fake', 'fake', 'real', 'fake', 'fake', 'fake', 'fake', 'real', 'real', 'real', 'fake', 'fake', 'real', 'real', 'real', 'real', 'fake', 'real', 'real', 'real', 'fake', 'real', 'real', 'fake', 'real', 'real', 'real', 'real', 'real', 'fake', 'real', 'fake', 'real', 'fake', 'fake', 'fake', 'fake', 'fake', 'real', 'fake', 'fake', 'fake', 'real', 'fake', 'real', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'real', 'real', 'real', 'fake', 'fake', 'real', 'real', 'real', 'real', 'fake', 'real', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'real', 'real', 'real', 'real', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'real', 'fake', 'real', 'fake', 'real', 'fake', 'fake', 'real', 'fake', 'fake', 'fake', 'fake', 'fake', 'real', 'fake', 'real', 'real', 'real', 'fake', 'real', 'fake', 'fake', 'real', 'fake', 'fake', 'real', 'fake', 'real', 'real', 'fake', 'fake', 'fake', 'fake', 'fake', 'real', 'fake', 'real', 'real', 'fake', 'real', 'real', 'fake', 'real', 'fake', 'fake', 'real', 'real', 'fake', 'real', 'real', 'fake', 'fake', 'fake', 'fake', 'real', 'real', 'fake', 'fake', 'real', 'real', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'real', 'fake', 'real', 'real', 'fake', 'real', 'fake', 'fake', 'real', 'fake', 'fake', 'real', 'real', 'fake', 'real', 'fake', 'real', 'fake', 'real', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'real', 'fake', 'real', 'real', 'real', 'real', 'real', 'fake', 'real', 'fake', 'real', 'real', 'fake', 'real', 'fake', 'real', 'real', 'fake', 'real', 'real', 'fake', 'fake', 'fake', 'real', 'real', 'real', 'fake', 'fake', 'fake', 'fake', 'real', 'real', 'fake', 'real', 'fake', 'fake', 'fake', 'real', 'real', 'fake', 'fake', 'fake', 'fake', 'real', 'fake', 'fake', 'real', 'real', 'real', 'fake', 'real', 'fake', 'fake', 'fake', 'fake', 'real', 'fake', 'real', 'real', 'fake', 'fake', 'fake', 'real', 'real', 'fake', 'real', 'real', 'fake', 'fake', 'fake', 'real', 'real', 'real', 'fake', 'real', 'real', 'real', 'fake', 'fake', 'fake', 'real', 'fake', 'fake', 'real', 'fake', 'fake', 'real', 'real', 'real', 'fake', 'real', 'real', 'fake', 'real', 'real', 'real', 'real', 'real', 'real', 'fake', 'fake', 'real', 'real', 'real', 'fake', 'real', 'real', 'real', 'fake', 'fake', 'real', 'fake', 'real', 'fake', 'fake', 'real', 'fake', 'fake', 'real', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'real', 'real', 'fake', 'real', 'real', 'fake', 'real', 'fake', 'real', 'real', 'real', 'fake', 'real', 'real', 'fake', 'fake', 'fake', 'real', 'real', 'fake', 'real', 'fake', 'fake', 'fake', 'real', 'real', 'fake', 'real', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake']\n"
     ]
    }
   ],
   "source": [
    "print(len(y_true))\n",
    "print(len(y_pred))\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "85706e81-3068-4150-9773-320a8aa98c69",
   "metadata": {
    "id": "85706e81-3068-4150-9773-320a8aa98c69",
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-05-07T18:08:26.619370Z",
     "start_time": "2024-05-07T18:08:26.499457Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600,)\n",
      "(1600,)\n",
      "(3200,)\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[   0, 1600],\n       [ 804,  796]], dtype=int64)"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creo i vettori di ground truth\n",
    "# y_true = np.array(['fake'] * 1523)\n",
    "y_true = np.array(['fake'] * 1600)\n",
    "print(y_true.shape)\n",
    "\n",
    "# temp = np.array(['real'] * 1523)\n",
    "temp = np.array(['real'] * 1600)\n",
    "print(temp.shape)\n",
    "\n",
    "# y_true = np.concatenate([y_true, temp])\n",
    "y_true = np.concatenate([y_true, temp])\n",
    "print(y_true.shape)\n",
    "\n",
    "# calcolo la matrice di confusione (quella di scikit-learn dispone i risultati come nella cella di sotto)\n",
    "confusion_matrix(y_true, y_pred, labels=[\"real\", \"fake\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "86e1ec24-b7d3-4264-9c89-fa882343fa19",
   "metadata": {
    "id": "86e1ec24-b7d3-4264-9c89-fa882343fa19",
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-05-07T18:08:26.730619Z",
     "start_time": "2024-05-07T18:08:26.621354Z"
    }
   },
   "outputs": [],
   "source": [
    "# estraggo dalla matrice di confusione i True Negative, False Positive, False Negative, True Positive\n",
    "TN, FP, FN, TP = confusion_matrix(y_true, y_pred, labels=[\"real\", \"fake\"]).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "28d903d0-38e3-4dc7-af71-f091c78a00e7",
   "metadata": {
    "id": "28d903d0-38e3-4dc7-af71-f091c78a00e7",
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-05-07T18:25:09.635490Z",
     "start_time": "2024-05-07T18:25:09.512474Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accuracy': 24.88, 'Precision': 33.22, 'Sensitivity_recall': 49.75, 'Specificity': 0.0, 'F1_score': 39.84}\n"
     ]
    }
   ],
   "source": [
    "# calcolo alcune metriche per vedere come si comporta\n",
    "accuracy = round((TP + TN) /(TP + TN + FP + FN), 4) * 100\n",
    "precision = round((TP) / (TP + FP), 4) * 100\n",
    "sensitivy_recall = round((TP) / (TP + FN), 4) * 100\n",
    "specificity = round((TN) / (TN + FP) * 100, 4)\n",
    "F1_score = round((2* precision * sensitivy_recall) / (precision + sensitivy_recall), 2)\n",
    "\n",
    "#Salvataggio del file csv\n",
    "df_result = pd.DataFrame({\"Precision\": [precision], \"Sensitivity\":[sensitivy_recall], \"Specificity\":[specificity],\"f1_score\":[F1_score]})\n",
    "output_dir_result = os.path.join(Path(os.getcwd()).parent, \"results\")\n",
    "output_dir_result = os.path.join(output_dir_result, \"results.csv\")\n",
    "df_result.to_csv(output_dir_result,index=False)\n",
    "\n",
    "print({\"Accuracy\":accuracy,\"Precision\":precision,\"Sensitivity_recall\":sensitivy_recall, \"Specificity\": specificity, \"F1_score\":F1_score})"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-07T18:20:24.706277Z",
     "start_time": "2024-05-07T18:20:24.688277Z"
    }
   },
   "id": "789e146e4ca73b13",
   "execution_count": 51
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "fvab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
