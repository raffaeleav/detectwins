{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "629f9a2a-51d3-4851-b6f2-a542601bb67e",
      "metadata": {
        "id": "629f9a2a-51d3-4851-b6f2-a542601bb67e",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import timm\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from torch import nn\n",
        "from tqdm import tqdm\n",
        "from skimage import io\n",
        "from random import sample\n",
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "477f08a1-15bb-471f-974c-23c6bef4ece3",
      "metadata": {
        "id": "477f08a1-15bb-471f-974c-23c6bef4ece3",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# classe per caricare il modello di rete neurale \n",
        "class ApnModel(nn.Module):\n",
        "\n",
        "  # size del vettore di embedding\n",
        "  def __init__(self, emb_size=512):\n",
        "    super(ApnModel, self).__init__()\n",
        "\n",
        "    # caricamento del modello, in questo caso efficientnet b0 (architettura più leggera della famiglia)\n",
        "    self.efficientnet = timm.create_model(\"tf_efficientnetv2_b0\", pretrained=False)\n",
        "    self.efficientnet.classifier = nn.Linear(in_features=self.efficientnet.classifier.in_features, out_features=emb_size)\n",
        "\n",
        "  def forward(self, images):\n",
        "    embeddings = self.efficientnet(images)\n",
        "    return embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a91a46c5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# funzione per generare i vettori di encoding\n",
        "def get_encoding_csv(model, anc_img_names, fake_data_dir, real_data_dir, device):\n",
        "  anc_img_names_arr = np.array(anc_img_names)\n",
        "  encodings = []\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for i in tqdm(anc_img_names_arr, desc=\"creating encodings...\"):\n",
        "      a = io.imread(i)\n",
        "      a = np.expand_dims(a, 0)\n",
        "      a = torch.from_numpy(a.astype(np.int32)) / 255.0\n",
        "      a = a.to(device)\n",
        "      \n",
        "      a_enc = model(a.unsqueeze(0))\n",
        "      encodings.append(a_enc.squeeze().cpu().detach().numpy())\n",
        "\n",
        "    encodings = np.array(encodings)\n",
        "    encodings = pd.DataFrame(encodings)\n",
        "    anc_img_names_df = pd.DataFrame(anc_img_names_arr, columns=['Anchor'])\n",
        "    df_enc = pd.concat([anc_img_names_df, encodings], axis=1)\n",
        "\n",
        "    return df_enc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2262d0bc-a58d-4663-8515-bccbdf870608",
      "metadata": {
        "id": "2262d0bc-a58d-4663-8515-bccbdf870608",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# funzione che genera embeddings di una singola immagine\n",
        "def get_image_embeddings(img, model, device):\n",
        "    img = np.expand_dims(img, 0)\n",
        "    img = torch.from_numpy(img) / 255\n",
        "    \n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        img = img.to(device)\n",
        "        img_enc = model(img.unsqueeze(0))\n",
        "        img_enc = img_enc.detach().cpu().numpy()\n",
        "        img_enc = np.array(img_enc)\n",
        "\n",
        "    return img_enc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e5e2844",
      "metadata": {},
      "outputs": [],
      "source": [
        "# distanza euclidea per array np\n",
        "def array_distance(img_enc, anc_enc_arr):\n",
        "    dist = np.dot(img_enc-anc_enc_arr, (img_enc- anc_enc_arr).T)\n",
        "    # dist = np.sqrt(dist)\n",
        "\n",
        "    return dist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b4b5bd4-222b-44a3-b32d-2aa1f9b3d405",
      "metadata": {
        "id": "0b4b5bd4-222b-44a3-b32d-2aa1f9b3d405",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# funzione che cerca nel database l'immagine più simile a quella data in input\n",
        "def search_in_database(img_enc, database):\n",
        "    anc_enc_arr = database.iloc[:, 1:].to_numpy()\n",
        "\n",
        "    distance = []\n",
        "    for i in range(anc_enc_arr.shape[0]):\n",
        "        dist = array_distance(img_enc, anc_enc_arr[i : i+1, :])\n",
        "        distance = np.append(distance, dist)\n",
        "\n",
        "    closest_idx = np.argsort(distance)\n",
        "\n",
        "    return database[\"Anchor\"][closest_idx[0]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d8d2204",
      "metadata": {},
      "outputs": [],
      "source": [
        "# funzione per ottenere i path di tutti i file in una cartella (per creare dataset di test)\n",
        "def get_file_paths(directory): \n",
        "    file_paths = []\n",
        "\n",
        "    for root, _, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            # path completo del file\n",
        "            file_path = os.path.join(root, file)  \n",
        "            file_paths.append(file_path)\n",
        "            \n",
        "    return file_paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fb6e0d0-b8cc-44b8-a8ac-17934de905f6",
      "metadata": {
        "id": "0fb6e0d0-b8cc-44b8-a8ac-17934de905f6",
        "tags": []
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f02ae726-f601-4b51-a949-71a5464ec779",
      "metadata": {
        "id": "f02ae726-f601-4b51-a949-71a5464ec779",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# configurazione\n",
        "device=\"cuda\"\n",
        "\n",
        "# stringhe usate nella funzione di testing, devono avere il nome uguale ai file delle immagini presenti nel database degli encodings\n",
        "# es. anchor=\"d:\\\\folder\\img_fake_01.png\" fake_dataset=\"fake\"\n",
        "# es. anchor=\"d:\\\\folder\\img_coco_01.png\" real_dataset=\"coco\"\n",
        "fake_dataset = \"taming_transformer\"\n",
        "real_dataset = \"coco\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2080916",
      "metadata": {},
      "outputs": [],
      "source": [
        "model = ApnModel()\n",
        "\n",
        "# per processare le immagini in scala di grigi per fare fourier serve una CNN 2D\n",
        "model.efficientnet.conv_stem = nn.Conv2d(1, 32, 3, 2, 1, bias=False)\n",
        "\n",
        "model.to(device);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "087c6f6f-3d8d-437f-ab31-ce2c3a1c239c",
      "metadata": {
        "id": "087c6f6f-3d8d-437f-ab31-ce2c3a1c239c",
        "outputId": "10e29b3a-1d0f-41bb-e9a2-21aec49dac69",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# per ricaricare il modello già allenato e il dataset di encoding (che servirà per il testing)\n",
        "model.load_state_dict(torch.load(\"best_model.pt\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18cd52ea",
      "metadata": {},
      "outputs": [],
      "source": [
        "# directory da dove vengono prelevate le immagini per il testing e per creare il dataset di encodings\n",
        "fake_data_dir = \"D:\\\\sviluppo\\\\project-detective\\\\temp\\\\taming_transformer+coco\\\\test\\\\taming_transformer\"\n",
        "real_data_dir = \"D:\\\\sviluppo\\\\project-detective\\\\temp\\\\taming_transformer+coco\\\\test\\\\coco\"\n",
        "anchor_data_dir = \"D:\\\\sviluppo\\\\project-detective\\\\temp\\\\taming_transformer+coco\\\\train\\\\taming_transformer\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6d62dd1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# su quante immagini per ogni classe si deve fare il test\n",
        "test_size = 100\n",
        "\n",
        "# si prelevano le immagini per fare il test\n",
        "fake_images = get_file_paths(fake_data_dir)\n",
        "real_images = get_file_paths(real_data_dir)\n",
        "\n",
        "fake_images = sample(fake_images, test_size)\n",
        "real_images = sample(real_images, test_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7882cc74",
      "metadata": {},
      "outputs": [],
      "source": [
        "# si crea il dataset di encodings\n",
        "anchor_images = get_file_paths(anchor_data_dir)\n",
        "anchor_df = pd.DataFrame(anchor_images, columns=[\"Anchor\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73d6a250",
      "metadata": {},
      "outputs": [],
      "source": [
        "# si creano gli embeddings che vengono memorizzati \n",
        "if not os.path.isfile(\"database.csv\"):\n",
        "    df_enc = get_encoding_csv(model, anchor_df[\"Anchor\"], fake_data_dir, real_data_dir, device)\n",
        "    df_enc.to_csv(\"database.csv\", index=False)\n",
        "else: \n",
        "    df_enc = pd.read_csv(\"database.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddd32725-5e2d-42a7-8998-4920f05ed143",
      "metadata": {
        "id": "ddd32725-5e2d-42a7-8998-4920f05ed143",
        "outputId": "888e6f94-a62a-46e1-cf29-d11664da20b7",
        "tags": []
      },
      "outputs": [],
      "source": [
        "y_true = []\n",
        "y_pred = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca7392f6-109f-4a96-92b1-e85600be8d8a",
      "metadata": {
        "id": "ca7392f6-109f-4a96-92b1-e85600be8d8a",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# testo i fake\n",
        "for i in tqdm(fake_images, desc=\"testing on fake images...\"):\n",
        "    path = i\n",
        "\n",
        "    # si legge l'immagine\n",
        "    img = io.imread(path)\n",
        "    # si ottiene il vettore di embeddings dell'immagine\n",
        "    img_enc = get_image_embeddings(img, model, device)\n",
        "    # si cerca nel dataset con gli encodings un'immagine simile \n",
        "    closest_label = search_in_database(img_enc, df_enc)\n",
        "    print(closest_label)\n",
        "    \n",
        "    # se nel path dell'immagine c'è il nome del dataset real è real\n",
        "    if real_dataset in str(closest_label):\n",
        "        y_pred.append(\"real\")\n",
        "    # viceversa\n",
        "    else:\n",
        "        y_pred.append(\"fake\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46b8f2c1-a3ca-4bbd-8326-02685fc44cf8",
      "metadata": {
        "id": "46b8f2c1-a3ca-4bbd-8326-02685fc44cf8",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# testo i real\n",
        "for i in tqdm(real_images, desc=\"testing on real images...\"):\n",
        "    path = i\n",
        "\n",
        "    img = io.imread(path)\n",
        "    img_enc = get_image_embeddings(img, model, device)\n",
        "    closest_label = search_in_database(img_enc, df_enc)\n",
        "    print(closest_label)\n",
        "\n",
        "    if real_dataset in str(closest_label):\n",
        "        y_pred.append(\"real\")\n",
        "    else:\n",
        "        y_pred.append(\"fake\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85706e81-3068-4150-9773-320a8aa98c69",
      "metadata": {
        "id": "85706e81-3068-4150-9773-320a8aa98c69",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# creo i vettori di ground truth\n",
        "y_true = np.array([\"fake\"] * len(fake_images))\n",
        "temp = np.array([\"real\"] * len(real_images))\n",
        "y_true = np.concatenate([y_true, temp])\n",
        "\n",
        "# calcolo la matrice di confusione \n",
        "cm = confusion_matrix(y_true, y_pred, labels=[\"real\", \"fake\"])\n",
        "print(cm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28d903d0-38e3-4dc7-af71-f091c78a00e7",
      "metadata": {
        "id": "28d903d0-38e3-4dc7-af71-f091c78a00e7",
        "tags": []
      },
      "outputs": [],
      "source": [
        "tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "# metriche\n",
        "accuracy = round((tp + tn) / (tp + tn + fp + fn), 4) * 100\n",
        "precision = round((tp) / (tp + fp), 4) * 100\n",
        "recall = round((tp) / (tp + fn), 4) * 100\n",
        "specificity = round((tn) / (tn + fp) * 100, 4)\n",
        "f1_score = round((2 * precision * recall) / (precision + recall), 4)\n",
        "\n",
        "print({\"Accuracy\":accuracy, \"Precision\":precision, \"Recall\":recall, \"Specificity\":specificity, \"F1 Score\":f1_score})"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "fvab",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
