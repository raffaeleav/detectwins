{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dccf7dfb",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "# si aggiunge al path la cartella utils per avere visibilità del module\n",
        "module_path = Path(os.getcwd()).parent.parent\n",
        "module_path = os.path.join(module_path, \"project-detective\")\n",
        "\n",
        "sys.path.append(module_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "629f9a2a-51d3-4851-b6f2-a542601bb67e",
      "metadata": {
        "id": "629f9a2a-51d3-4851-b6f2-a542601bb67e",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import timm\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import utils.datasets as datasets\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "from torch import nn\n",
        "from tqdm import tqdm\n",
        "from skimage import io\n",
        "from random import sample\n",
        "from sklearn.metrics import confusion_matrix, PrecisionRecallDisplay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18cd52ea",
      "metadata": {},
      "outputs": [],
      "source": [
        "# directory da dove vengono prelevate le immagini per il testing\n",
        "# assicurarsi che i path siano con i doppi backslash (\\\\) es. \"D:\\\\sviluppo\\\\artifact\\\\diffusion_gan\\\\diff\\\\afhq-data\\\\Diffusion-StyleGAN2-ADA-AFHQ\"\n",
        "fake_data_dir = \"\"\n",
        "real_data_dir = \"\"\n",
        "\n",
        "# path del dataset artifact che serve per creare il database per il testing\n",
        "artifact_path = \"D:\\\\sviluppo\\\\artifact\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "477f08a1-15bb-471f-974c-23c6bef4ece3",
      "metadata": {
        "id": "477f08a1-15bb-471f-974c-23c6bef4ece3",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# classe per caricare il modello di rete neurale \n",
        "class ApnModel(nn.Module):\n",
        "\n",
        "  # size del vettore di embedding\n",
        "  def __init__(self, emb_size=512):\n",
        "    super(ApnModel, self).__init__()\n",
        "\n",
        "    # caricamento del modello, in questo caso efficientnet b0 (architettura più leggera della famiglia)\n",
        "    self.efficientnet = timm.create_model(\"tf_efficientnetv2_b0\", pretrained=False)\n",
        "    self.efficientnet.classifier = nn.Linear(in_features=self.efficientnet.classifier.in_features, out_features=emb_size)\n",
        "\n",
        "  def forward(self, images):\n",
        "    embeddings = self.efficientnet(images)\n",
        "    return embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a91a46c5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# funzione per generare i vettori di encoding\n",
        "def get_encoding_csv(model, anc_img_names, fake_data_dir, real_data_dir, device):\n",
        "  anc_img_names_arr = np.array(anc_img_names)\n",
        "  encodings = []\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for i in tqdm(anc_img_names_arr, desc=\"creating encodings...\"):\n",
        "      if \"real\" in str(i):\n",
        "        dir_folder = real_data_dir\n",
        "      else: \n",
        "        dir_folder = fake_data_dir\n",
        "\n",
        "      a = io.imread(os.path.join(dir_folder, i))\n",
        "      a = np.expand_dims(a, 0)\n",
        "      a = torch.from_numpy(a.astype(np.int32)) / 255.0\n",
        "      a = a.to(device)\n",
        "      \n",
        "      a_enc = model(a.unsqueeze(0))\n",
        "      encodings.append(a_enc.squeeze().cpu().detach().numpy())\n",
        "\n",
        "    encodings = np.array(encodings)\n",
        "    encodings = pd.DataFrame(encodings)\n",
        "    anc_img_names_df = pd.DataFrame(anc_img_names_arr, columns=['Anchor'])\n",
        "    df_enc = pd.concat([anc_img_names_df, encodings], axis=1)\n",
        "\n",
        "    return df_enc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2262d0bc-a58d-4663-8515-bccbdf870608",
      "metadata": {
        "id": "2262d0bc-a58d-4663-8515-bccbdf870608",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# funzione che genera embeddings di una singola immagine\n",
        "def get_image_embeddings(img, model, device):\n",
        "    img = np.expand_dims(img, 0)\n",
        "    img = torch.from_numpy(img) / 255\n",
        "    \n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        img = img.to(device)\n",
        "        img_enc = model(img.unsqueeze(0))\n",
        "        img_enc = img_enc.detach().cpu().numpy()\n",
        "        img_enc = np.array(img_enc)\n",
        "\n",
        "    return img_enc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e5e2844",
      "metadata": {},
      "outputs": [],
      "source": [
        "# distanza euclidea per array np\n",
        "def array_distance(img_enc, anc_enc_arr):\n",
        "    dist = np.dot(img_enc-anc_enc_arr, (img_enc- anc_enc_arr).T)\n",
        "    # dist = np.sqrt(dist)\n",
        "\n",
        "    return dist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b4b5bd4-222b-44a3-b32d-2aa1f9b3d405",
      "metadata": {
        "id": "0b4b5bd4-222b-44a3-b32d-2aa1f9b3d405",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# funzione che cerca nel database l'immagine più simile a quella data in input\n",
        "def search_in_database(img_enc, database):\n",
        "    anc_enc_arr = database.iloc[:, 1:].to_numpy()\n",
        "\n",
        "    distance = []\n",
        "    for i in range(anc_enc_arr.shape[0]):\n",
        "        dist = array_distance(img_enc, anc_enc_arr[i : i+1, :])\n",
        "        distance = np.append(distance, dist)\n",
        "\n",
        "    closest_idx = np.argsort(distance)\n",
        "\n",
        "    return database[\"Anchor\"][closest_idx[0]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d8d2204",
      "metadata": {},
      "outputs": [],
      "source": [
        "# funzione per ottenere i path di tutti i file in una cartella (per creare dataset di test)\n",
        "def get_file_paths(directory): \n",
        "    file_paths = []\n",
        "\n",
        "    for root, _, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            # path completo del file\n",
        "            file_path = os.path.join(root, file)  \n",
        "            file_paths.append(file_path)\n",
        "            \n",
        "    return file_paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fb6e0d0-b8cc-44b8-a8ac-17934de905f6",
      "metadata": {
        "id": "0fb6e0d0-b8cc-44b8-a8ac-17934de905f6",
        "tags": []
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f02ae726-f601-4b51-a949-71a5464ec779",
      "metadata": {
        "id": "f02ae726-f601-4b51-a949-71a5464ec779",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# configurazione\n",
        "device=\"cuda\"\n",
        "\n",
        "# stringhe usate nella funzione di testing, devono avere il nome uguale ai file delle immagini presenti nel database degli encodings\n",
        "# es. anchor=\"d:\\\\folder\\img_fake_01.png\" fake_dataset=\"fake\"\n",
        "# es. anchor=\"d:\\\\folder\\img_coco_01.png\" real_dataset=\"coco\"\n",
        "fake_dataset = \"fake\"\n",
        "real_dataset = \"real\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2080916",
      "metadata": {},
      "outputs": [],
      "source": [
        "model = ApnModel()\n",
        "\n",
        "# per processare le immagini in scala di grigi per fare fourier serve una CNN 2D\n",
        "model.efficientnet.conv_stem = nn.Conv2d(1, 32, 3, 2, 1, bias=False)\n",
        "\n",
        "model.to(device);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "087c6f6f-3d8d-437f-ab31-ce2c3a1c239c",
      "metadata": {
        "id": "087c6f6f-3d8d-437f-ab31-ce2c3a1c239c",
        "outputId": "10e29b3a-1d0f-41bb-e9a2-21aec49dac69",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# per ricaricare il modello già allenato e il dataset di encoding (che servirà per il testing)\n",
        "model.load_state_dict(torch.load(\"best_model.pt\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6d62dd1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# su quante immagini per ogni classe si deve fare il test\n",
        "test_size = 1000\n",
        "\n",
        "# si prelevano le immagini per fare il test\n",
        "fake_images_paths = get_file_paths(fake_data_dir)\n",
        "real_images_paths = get_file_paths(real_data_dir)\n",
        "\n",
        "fake_images = sample(fake_images_paths, test_size)\n",
        "real_images = sample(real_images_paths, test_size * 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "id": "6ea1ebfd",
      "metadata": {},
      "outputs": [],
      "source": [
        "if not os.path.isdir(os.path.join(\"..\", \"temp\")):\n",
        "    datasets.fourier(\"taming_transformer\", \"coco\", artifact_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7882cc74",
      "metadata": {},
      "outputs": [],
      "source": [
        "# cartelle dove prelevare le immagini del dataset di training \n",
        "fake_df_out_dir = os.path.join(\"..\", \"temp\", \"taming_transformer+coco\", \"train\", \"taming_transformer\")\n",
        "real_df_out_dir = os.path.join(\"..\", \"temp\", \"taming_transformer+coco\", \"train\", \"coco\")\n",
        "\n",
        "# si usa il dataset di training per creare il dataset di encodings che serve per il testing\n",
        "df_out_path = os.path.join(\"..\", \"datasets\", \"fourier_out.csv\")\n",
        "df_out = pd.read_csv(df_out_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73d6a250",
      "metadata": {},
      "outputs": [],
      "source": [
        "# si creano gli embeddings che vengono memorizzati \n",
        "if not os.path.isfile(\"database.csv\"):\n",
        "    df_enc = get_encoding_csv(model, df_out[\"Anchor\"], fake_df_out_dir, real_df_out_dir, device)\n",
        "    df_enc.to_csv(\"database.csv\", index=False)\n",
        "else: \n",
        "    df_enc = pd.read_csv(\"database.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddd32725-5e2d-42a7-8998-4920f05ed143",
      "metadata": {
        "id": "ddd32725-5e2d-42a7-8998-4920f05ed143",
        "outputId": "888e6f94-a62a-46e1-cf29-d11664da20b7",
        "tags": []
      },
      "outputs": [],
      "source": [
        "y_true = []\n",
        "y_pred = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca7392f6-109f-4a96-92b1-e85600be8d8a",
      "metadata": {
        "id": "ca7392f6-109f-4a96-92b1-e85600be8d8a",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# testo i fake\n",
        "for i in tqdm(fake_images, desc=\"testing on fake images...\"):\n",
        "    path = i\n",
        "\n",
        "    # si legge l'immagine\n",
        "    img = io.imread(path)\n",
        "    # si ottiene il vettore di embeddings dell'immagine\n",
        "    img_enc = get_image_embeddings(img, model, device)\n",
        "    # si cerca nel dataset con gli encodings un'immagine simile \n",
        "    closest_label = search_in_database(img_enc, df_enc)\n",
        "    \n",
        "    # se nel path dell'immagine c'è il nome del dataset real è real\n",
        "    if real_dataset in str(closest_label):\n",
        "        y_pred.append(\"real\")\n",
        "    # viceversa\n",
        "    else:\n",
        "        y_pred.append(\"fake\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46b8f2c1-a3ca-4bbd-8326-02685fc44cf8",
      "metadata": {
        "id": "46b8f2c1-a3ca-4bbd-8326-02685fc44cf8",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# testo i real\n",
        "for i in tqdm(real_images, desc=\"testing on real images...\"):\n",
        "    path = i\n",
        "\n",
        "    img = io.imread(path)\n",
        "    img_enc = get_image_embeddings(img, model, device)\n",
        "    closest_label = search_in_database(img_enc, df_enc)\n",
        "\n",
        "    if real_dataset in str(closest_label):\n",
        "        y_pred.append(\"real\")\n",
        "    else:\n",
        "        y_pred.append(\"fake\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85706e81-3068-4150-9773-320a8aa98c69",
      "metadata": {
        "id": "85706e81-3068-4150-9773-320a8aa98c69",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# creo i vettori di ground truth\n",
        "y_true = np.array([\"fake\"] * len(fake_images))\n",
        "temp = np.array([\"real\"] * len(real_images))\n",
        "y_true = np.concatenate([y_true, temp])\n",
        "\n",
        "# calcolo la matrice di confusione \n",
        "cm = confusion_matrix(y_true, y_pred, labels=[\"real\", \"fake\"])\n",
        "print(cm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28d903d0-38e3-4dc7-af71-f091c78a00e7",
      "metadata": {
        "id": "28d903d0-38e3-4dc7-af71-f091c78a00e7",
        "tags": []
      },
      "outputs": [],
      "source": [
        "tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "# metriche\n",
        "accuracy = round((tp + tn) / (tp + tn + fp + fn), 4) * 100\n",
        "precision = round((tp) / (tp + fp), 4) * 100\n",
        "recall = round((tp) / (tp + fn), 4) * 100\n",
        "specificity = round((tn) / (tn + fp) * 100, 4)\n",
        "f1_score = round((2 * precision * recall) / (precision + recall), 4)\n",
        "\n",
        "print({\"Accuracy\":accuracy, \"Precision\":precision, \"Recall\":recall, \"Specificity\":specificity, \"F1 Score\":f1_score})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5a45ec6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# si convertono ground truth e predizioni in binario per calcolare la precision-recall curve\n",
        "y_pred_nd = np.array(y_pred)\n",
        "\n",
        "y_true_binary = np.where(y_true == \"real\", 0, 1)\n",
        "y_pred_binary = np.where(y_pred_nd == \"real\", 0, 1)\n",
        "\n",
        "display = PrecisionRecallDisplay.from_predictions(\n",
        "    y_true_binary, y_pred_binary, pos_label=1, name=\"Online Hard Mining + Fourier\", plot_chance_level=True\n",
        ")\n",
        "\n",
        "display.ax_.set_title(\"Precision-Recall curve\");"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "fvab",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
