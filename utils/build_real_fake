import itertools
import os
import pandas as pd
import torch
from tqdm import tqdm
from pathlib import Path

def build_real_fake_dataframe(tt_path, ld_path, bg_path, coco_path, real_output_dir, fake_output_dir):

    taming_transformer = pd.read_csv(tt_path, usecols=['image_path'], nrows=70000)
    latent_diffusion = pd.read_csv(ld_path, usecols=['image_path'], nrows=20000)
    big_gan = pd.read_csv(bg_path, usecols=['image_path'], nrows=10000)
    coco = pd.read_csv(coco_path, usecols=['image_path'], nrows=110000)

    df_fake = pd.concat([taming_transformer, latent_diffusion, big_gan], ignore_index=True)
     
    real_df = pd.DataFrame({'real': coco['image_path']})
    real_df.to_csv(real_output_dir, index=False)
    fake_df = pd.DataFrame({'fake': df_fake['image_path']})
    fake_df.to_csv(fake_output_dir, index=False)


# attenzione ai path inseriti
def main():
    path = Path(__file__).parent.parent.parent
    project_path = Path(__file__).parent.parent

    artifact_path = os.path.join(path, "artifact")
    tt_dataset_path = os.path.join(artifact_path, "taming_transformer", "metadata.csv")
    ld_dataset_path = os.path.join(artifact_path, "latent_diffusion", "metadata.csv")
    bg_dataset_path = os.path.join(artifact_path, "big_gan", "metadata.csv")
    real_dataset_path = os.path.join(artifact_path, "coco", "metadata.csv")

    real_output_dir = os.path.join(project_path, "datasets", "real.csv")
    fake_output_dir = os.path.join(project_path, "datasets", "fake.csv")

    build_real_fake_dataframe(tt_dataset_path, ld_dataset_path, bg_dataset_path, real_dataset_path, real_output_dir, fake_output_dir)


if __name__ == "__main__":
    main()